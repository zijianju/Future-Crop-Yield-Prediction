{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLCC Project Code for Ctrl+Alt+Deheat"
      ],
      "metadata": {
        "id": "uejvI64SwZ5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please take note that our tree based models were ran on Kaggle and our LSTM based ANNs were done locally/via Google Colab and Google Drive. If to recompile, multiple environemnt setup is required. Thus this merely serves as a placeholder for all our different notebooks. None of the code is actually compiled in this notebook here. For the figure and plotting results, please refer to the individual notebooks."
      ],
      "metadata": {
        "id": "fHFyZXGGwgOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest (Maize)"
      ],
      "metadata": {
        "id": "Uas-0vyrw8Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "pS5QGKLbwefj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "4LupsvMpxEu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "hQFnncN0xIOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_future_crop_challenge_path = kagglehub.competition_download('the-future-crop-challenge')\n",
        "print(\"data import successfully\")"
      ],
      "metadata": {
        "id": "kDiRijxrxmAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(crop: str, mode: str=\"train\"):\n",
        "    base_path = the_future_crop_challenge_path\n",
        "\n",
        "    tasmax = pd.read_parquet(f\"{base_path}/tasmax_{crop}_{mode}.parquet\")\n",
        "    tasmin = pd.read_parquet(f\"{base_path}/tasmin_{crop}_{mode}.parquet\")\n",
        "    pr = pd.read_parquet(f\"{base_path}/pr_{crop}_{mode}.parquet\")\n",
        "    rsds = pd.read_parquet(f\"{base_path}/rsds_{crop}_{mode}.parquet\")\n",
        "    soil_co2 = pd.read_parquet(f\"{base_path}/soil_co2_{crop}_{mode}.parquet\")\n",
        "\n",
        "    target = None\n",
        "    if mode == \"train\":\n",
        "        target = pd.read_parquet(f\"{base_path}/train_solutions_{crop}.parquet\")\n",
        "\n",
        "    return {\n",
        "        'tasmax': tasmax,\n",
        "        'tasmin': tasmin,\n",
        "        'pr': pr,\n",
        "        'rsds': rsds,\n",
        "        'soil_co2': soil_co2,\n",
        "        'target': target,\n",
        "    }"
      ],
      "metadata": {
        "id": "JRCpNac3xpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data for maize\n",
        "maize_train = load_data(\"maize\", \"train\")"
      ],
      "metadata": {
        "id": "zQ71vuHCxrqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_daily_data(df):\n",
        "    numeric_df = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
        "    return numeric_df.agg(['mean', 'max', 'min', 'std'], axis=1)\n",
        "tasmax_agg_maize = aggregate_daily_data(maize_train['tasmax'])\n",
        "tasmin_agg_maize = aggregate_daily_data(maize_train['tasmin'])\n",
        "pr_agg_maize = aggregate_daily_data(maize_train['pr'])\n",
        "rsds_agg_maize = aggregate_daily_data(maize_train['rsds'])\n",
        "tasmax_agg_maize.columns = [f'tasmax_{col}' for col in tasmax_agg_maize.columns]\n",
        "tasmin_agg_maize.columns = [f'tasmin_{col}' for col in tasmin_agg_maize.columns]\n",
        "pr_agg_maize.columns = [f'pr_{col}' for col in pr_agg_maize.columns]\n",
        "rsds_agg_maize.columns = [f'rsds_{col}' for col in rsds_agg_maize.columns]\n",
        "\n",
        "feature_maize = pd.concat([tasmax_agg_maize, tasmin_agg_maize, pr_agg_maize, rsds_agg_maize, maize_train['soil_co2']], axis = 1)\n",
        "target_maize = maize_train['target']"
      ],
      "metadata": {
        "id": "VGr53-8yxsTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maize"
      ],
      "metadata": {
        "id": "97jjsmRHxtxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop non-numeric columns\n",
        "numeric_features_maize = feature_maize.select_dtypes(include=[np.number])\n",
        "\n",
        "# Scale features\n",
        "scaler_maize = StandardScaler()\n",
        "features_scaled_maize = scaler_maize.fit_transform(numeric_features_maize)"
      ],
      "metadata": {
        "id": "TybmDB9wxvCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_series = maize_train['tasmax']['year']"
      ],
      "metadata": {
        "id": "C7EI-GE1xwrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Find validation year (2020)\n",
        "val_year = year_series.max()  # Should be 2020\n",
        "print(f\"Using year {val_year} for validation.\") # Note that this printout does not resemble the actual year of 2020"
      ],
      "metadata": {
        "id": "9oiRHtvvxy_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_mask = (year_series == val_year)\n",
        "\n",
        "# Step 4: Split data\n",
        "X_train = features_scaled_maize[~val_mask]\n",
        "X_val = features_scaled_maize[val_mask]\n",
        "y_train = target_maize[~val_mask].values.ravel()\n",
        "y_val = target_maize[val_mask].values.ravel()"
      ],
      "metadata": {
        "id": "uXeDLEvTx6kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_lXM6vXvx8IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = rf_model.predict(X_val)\n",
        "\n",
        "# Step 7: Evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Random Forest Validation RMSE (2020): {rmse:.4f}\")\n",
        "print(f\"Random Forest Validation R2 (2020): {r2:.4f}\")"
      ],
      "metadata": {
        "id": "geWH6oD3x8nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted vs Actual Scatter Plot (for 2020)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_val, y=y_val_pred, alpha=0.6, edgecolor=None)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield (2020)\")\n",
        "plt.ylabel(\"Predicted Yield (2020)\")\n",
        "plt.title(f\" Random Forest: Predicted vs Actual Maize Yield (2020)\\nR² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mfHC3tmMx-wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals (Actual - Predicted)\n",
        "residuals = y_val - y_val_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(residuals, kde=True, bins=30, color='orange')\n",
        "plt.title(\"Random Forest Residuals (2020): Actual - Predicted (Maize)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FQWEN7EUyhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "maize_test = load_data(\"maize\", \"test\")"
      ],
      "metadata": {
        "id": "HllNjO6Nyh5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maize_test = load_data(\"maize\", \"test\")\n",
        "\n",
        "tasmax_agg_test = aggregate_daily_data(maize_test['tasmax'])\n",
        "tasmax_agg_test.columns = [f'tasmax_{col}' for col in tasmax_agg_test.columns]\n",
        "\n",
        "tasmin_agg_test = aggregate_daily_data(maize_test['tasmin'])\n",
        "tasmin_agg_test.columns = [f'tasmin_{col}' for col in tasmin_agg_test.columns]\n",
        "\n",
        "pr_agg_test = aggregate_daily_data(maize_test['pr'])\n",
        "pr_agg_test.columns = [f'pr_{col}' for col in pr_agg_test.columns]\n",
        "\n",
        "rsds_agg_test = aggregate_daily_data(maize_test['rsds'])\n",
        "rsds_agg_test.columns = [f'rsds_{col}' for col in rsds_agg_test.columns]\n",
        "\n",
        "features_test = pd.concat([tasmax_agg_test, tasmin_agg_test, pr_agg_test, rsds_agg_test, maize_test['soil_co2']], axis=1)\n",
        "\n",
        "numeric_features_test = features_test.select_dtypes(include=[np.number])\n",
        "\n",
        "numeric_features_test = numeric_features_test.dropna()\n",
        "\n",
        "features_scaled_test = scaler_maize.transform(numeric_features_test)\n",
        "test_ids = numeric_features_test.index"
      ],
      "metadata": {
        "id": "ZUQqwWUiykHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_predictions = rf_model.predict(features_scaled_test)\n",
        "rf_results = pd.DataFrame({\n",
        "    \"ID\": test_ids,\n",
        "    \"Predicted_Yield\": rf_predictions\n",
        "})\n",
        "rf_results.head()"
      ],
      "metadata": {
        "id": "3ruDq4BIylm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Year decoder function\n",
        "def decode_year(encoded_year):\n",
        "    return encoded_year - 381 + 1980\n",
        "\n",
        "# Step 1: Random Forest predictions (2021–2100), with decoded years\n",
        "rf_years_decoded = decode_year(maize_test['tasmax'].loc[numeric_features_test.index, 'year']).astype(int)\n",
        "\n",
        "rf_pred_df = pd.DataFrame({\n",
        "    \"Year\": rf_years_decoded,\n",
        "    \"Yield\": rf_predictions\n",
        "}, index=numeric_features_test.index)\n",
        "\n",
        "rf_future = rf_pred_df[(rf_pred_df[\"Year\"] >= 2021) & (rf_pred_df[\"Year\"] <= 2100)]\n",
        "rf_future_avg = rf_future.groupby(\"Year\", as_index=False)[\"Yield\"].mean().rename(columns={\"Yield\": \"Yield_pred\"})\n",
        "\n",
        "# Step 2: Historical yield (1980–2020), reuse val_mask and y_train/y_val from earlier RF setup\n",
        "encoded_years = maize_train[\"tasmax\"][\"year\"]\n",
        "\n",
        "# Decode training and validation years\n",
        "train_years = decode_year(encoded_years[~val_mask].astype(int))\n",
        "val_years = decode_year(encoded_years[val_mask].astype(int))\n",
        "\n",
        "historical_years_all = pd.concat([train_years.reset_index(drop=True), val_years.reset_index(drop=True)])\n",
        "historical_yields_all = np.concatenate([y_train, y_val])\n",
        "\n",
        "historical_df = pd.DataFrame({\n",
        "    \"Year\": historical_years_all,\n",
        "    \"Yield_hist\": historical_yields_all\n",
        "})\n",
        "historical_avg = historical_df.groupby(\"Year\", as_index=False)[\"Yield_hist\"].mean()\n",
        "\n",
        "# Step 3: Merge into full 1980–2100 DataFrame\n",
        "all_years = pd.DataFrame({\"Year\": range(1980, 2101)})\n",
        "historical_avg[\"Year\"] = historical_avg[\"Year\"].astype(int)\n",
        "rf_future_avg[\"Year\"] = rf_future_avg[\"Year\"].astype(int)\n",
        "\n",
        "plot_df = all_years.merge(historical_avg, on=\"Year\", how=\"left\")\n",
        "plot_df = plot_df.merge(rf_future_avg, on=\"Year\", how=\"left\")\n",
        "\n",
        "# Step 4: Plot Random Forest line (same style as XGBoost version)\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_hist\"].notna()],\n",
        "         plot_df[\"Yield_hist\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:blue\", label=\"Historical Yield (1980–2020)\")\n",
        "\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_pred\"].notna()],\n",
        "         plot_df[\"Yield_pred\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:orange\", label=\"Random Forest Predicted Yield (2021–2100)\")\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Random Forest Maize Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.ylim(2.96)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JMQSMgd4yoGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine historical and predicted yields into one continuous series\n",
        "combined_yield = pd.concat([\n",
        "    plot_df.set_index(\"Year\")[\"Yield_hist\"].dropna(),\n",
        "    plot_df.set_index(\"Year\")[\"Yield_pred\"].dropna()\n",
        "])\n",
        "\n",
        "# Plot the full timeline\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(combined_yield.index, combined_yield.values, marker=\"o\", color=\"tab:purple\", label=\"Random Forest: Historical + Predicted Yield\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield (t/ha) across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Random Forest Maize Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.ylim(3.0, combined_yield.max() * 1.05)  # Start y-axis at 3.0\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r-0u8cTXy2Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from XGBoost\n",
        "importances = rf_model.feature_importances_\n",
        "features = numeric_features_maize.columns\n",
        "sorted_idx = np.argsort(importances)[::-1]  # Descending order\n",
        "top_idx = sorted_idx[:10]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importances[top_idx], y=features[top_idx], ci=None)\n",
        "plt.title(\"Feature Importance from Random Forest(Maize)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3wp1ICV7y3Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "features = numeric_features_maize.columns\n",
        "\n",
        "# Create a DataFrame of features and their importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance descending\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display top N (optional)\n",
        "importance_df.head(20)"
      ],
      "metadata": {
        "id": "yh7TLupby6CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare historical and predicted yield DataFrames (same structure as XGBoost version)\n",
        "historical_rf_df = historical_df[[\"Year\", \"Yield_hist\"]].rename(columns={\"Yield_hist\": \"Yield\"})\n",
        "historical_rf_df = historical_rf_df[(historical_rf_df[\"Year\"] >= 1980) & (historical_rf_df[\"Year\"] <= 2020)]\n",
        "\n",
        "future_rf_df = rf_pred_df[rf_pred_df[\"Year\"] >= 2021]\n",
        "\n",
        "# KDE plot for yield distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(data=historical_rf_df, x=\"Yield\", label=\"1980–2020\", fill=True)\n",
        "sns.kdeplot(data=future_rf_df, x=\"Yield\", label=\"2021–2100\", fill=True)\n",
        "plt.title(\"Maize Yield Distribution: Historical vs Predicted (Random Forest)\")\n",
        "plt.xlabel(\"Yield\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XQ_G01B9y6DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest (Wheat)"
      ],
      "metadata": {
        "id": "2RKGCEOH0TSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "ykKYRL-K0Xfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def load_data(crop: str, mode: str=\"train\"):\n",
        "    base_path = \"/kaggle/input/the-future-crop-challenge\"\n",
        "\n",
        "    tasmax = pd.read_parquet(f\"{base_path}/tasmax_{crop}_{mode}.parquet\")\n",
        "    tasmin = pd.read_parquet(f\"{base_path}/tasmin_{crop}_{mode}.parquet\")\n",
        "    pr = pd.read_parquet(f\"{base_path}/pr_{crop}_{mode}.parquet\")\n",
        "    rsds = pd.read_parquet(f\"{base_path}/rsds_{crop}_{mode}.parquet\")\n",
        "    soil_co2 = pd.read_parquet(f\"{base_path}/soil_co2_{crop}_{mode}.parquet\")\n",
        "\n",
        "    target = None\n",
        "    if mode == \"train\":\n",
        "        target = pd.read_parquet(f\"{base_path}/train_solutions_{crop}.parquet\")\n",
        "\n",
        "    return {\n",
        "        'tasmax': tasmax,\n",
        "        'tasmin': tasmin,\n",
        "        'pr': pr,\n",
        "        'rsds': rsds,\n",
        "        'soil_co2': soil_co2,\n",
        "        'target': target,\n",
        "    }\n",
        "\n",
        "wheat_train = load_data(\"wheat\", \"train\")"
      ],
      "metadata": {
        "id": "FYedQog10Z07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_train['tasmax'].head()"
      ],
      "metadata": {
        "id": "jDydl_qD2CpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_daily_data(df):\n",
        "    numeric_df = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
        "    return numeric_df.agg(['mean', 'max', 'min', 'std'], axis=1)\n",
        "\n",
        "tasmax_agg_wheat = aggregate_daily_data(wheat_train['tasmax'])\n",
        "tasmin_agg_wheat = aggregate_daily_data(wheat_train['tasmin'])\n",
        "pr_agg_wheat = aggregate_daily_data(wheat_train['pr'])\n",
        "rsds_agg_wheat = aggregate_daily_data(wheat_train['rsds'])\n",
        "\n",
        "tasmax_agg_wheat.columns = [f'tasmax_{col}' for col in tasmax_agg_wheat.columns]\n",
        "tasmin_agg_wheat.columns = [f'tasmin_{col}' for col in tasmin_agg_wheat.columns]\n",
        "pr_agg_wheat.columns     = [f'pr_{col}' for col in pr_agg_wheat.columns]\n",
        "rsds_agg_wheat.columns   = [f'rsds_{col}' for col in rsds_agg_wheat.columns]\n",
        "\n",
        "features_wheat = pd.concat([tasmax_agg_wheat, tasmin_agg_wheat, pr_agg_wheat, rsds_agg_wheat, wheat_train['soil_co2']], axis=1)\n",
        "target_wheat = wheat_train['target']"
      ],
      "metadata": {
        "id": "plXI9UvV2EgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_wheat"
      ],
      "metadata": {
        "id": "AsOzysNJ2F9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_wheat"
      ],
      "metadata": {
        "id": "uuB4LyLE2H1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features_wheat = features_wheat.select_dtypes(include=[np.number])\n",
        "\n",
        "scaler_wheat = StandardScaler()\n",
        "features_scaled_wheat = scaler_wheat.fit_transform(numeric_features_wheat)"
      ],
      "metadata": {
        "id": "FOzW1KFr2I3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_series = wheat_train['tasmax']['year']\n",
        "val_year = year_series.max()  # Should be 2020\n",
        "print(f\"Using year {val_year} for validation.\")"
      ],
      "metadata": {
        "id": "qAfhY7ws2KwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_mask = (year_series == val_year)\n",
        "X_train = features_scaled_wheat[~val_mask]\n",
        "X_val = features_scaled_wheat[val_mask]\n",
        "y_train = target_wheat[~val_mask].values.ravel()\n",
        "y_val = target_wheat[val_mask].values.ravel()"
      ],
      "metadata": {
        "id": "DUMZwyYx2L8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# model = RandomForestRegressor(\n",
        "#     n_estimators=1,\n",
        "#     warm_start=True,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# for i in tqdm(range(1, 101), desc=\"Training Random Forest\"):\n",
        "#     model.set_params(n_estimators=i)\n",
        "#     model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OuAbpzvd2M-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "y_val_pred = model.predict(X_val)\n",
        "\n",
        "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Random Forest Validation RMSE (2020): {rmse:.4f}\")\n",
        "print(f\"Random Forest Validation R2 (2020): {r2:.4f}\")"
      ],
      "metadata": {
        "id": "KedoZYCg2QAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_val, y=y_val_pred, alpha=0.6, edgecolor=None)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield (2020)\")\n",
        "plt.ylabel(\"Predicted Yield (2020)\")\n",
        "plt.title(f\" Random Forest: Predicted vs Actual Wheat Yield (2020)\\nR² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3Sl-ghm2RYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals (Actual - Predicted)\n",
        "residuals = y_val - y_val_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(residuals, kde=True, bins=30, color='orange')\n",
        "plt.title(\"Random Forest Residuals (2020): Actual - Predicted (Wheat)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0iqOeGt2TeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data and do the same\n",
        "wheat_test = load_data(\"wheat\", \"test\")\n",
        "\n",
        "tasmax_agg_test = aggregate_daily_data(wheat_test['tasmax'])\n",
        "tasmax_agg_test.columns = [f'tasmax_{col}' for col in tasmax_agg_test.columns]\n",
        "\n",
        "tasmin_agg_test = aggregate_daily_data(wheat_test['tasmin'])\n",
        "tasmin_agg_test.columns = [f'tasmin_{col}' for col in tasmin_agg_test.columns]\n",
        "\n",
        "pr_agg_test = aggregate_daily_data(wheat_test['pr'])\n",
        "pr_agg_test.columns = [f'pr_{col}' for col in pr_agg_test.columns]\n",
        "\n",
        "rsds_agg_test = aggregate_daily_data(wheat_test['rsds'])\n",
        "rsds_agg_test.columns = [f'rsds_{col}' for col in rsds_agg_test.columns]\n",
        "\n",
        "features_test = pd.concat([tasmax_agg_test, tasmin_agg_test, pr_agg_test, rsds_agg_test, wheat_test['soil_co2']], axis=1)\n",
        "\n",
        "numeric_features_test = features_test.select_dtypes(include=[np.number])\n",
        "\n",
        "numeric_features_test = numeric_features_test.dropna()\n",
        "\n",
        "features_scaled_test = scaler_wheat.transform(numeric_features_test)\n",
        "test_ids = numeric_features_test.index\n",
        "\n",
        "predictions = model.predict(features_scaled_test)\n",
        "pred_results = pd.DataFrame({\n",
        "    \"ID\": test_ids,\n",
        "    \"Predicted_Yield\": predictions\n",
        "})\n",
        "pred_results.head()"
      ],
      "metadata": {
        "id": "eTE-KKFB2UkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Year decoder function\n",
        "def decode_year(encoded_year):\n",
        "    return encoded_year - 381 + 1980\n",
        "\n",
        "# Step 1: Random Forest predictions (2021–2100), with decoded years\n",
        "rf_years_decoded = decode_year(wheat_test['tasmax'].loc[numeric_features_test.index, 'year']).astype(int)\n",
        "\n",
        "rf_pred_df = pd.DataFrame({\n",
        "    \"Year\": rf_years_decoded,\n",
        "    \"Yield\": predictions\n",
        "}, index=numeric_features_test.index)\n",
        "\n",
        "rf_future = rf_pred_df[(rf_pred_df[\"Year\"] >= 2021) & (rf_pred_df[\"Year\"] <= 2100)]\n",
        "rf_future_avg = rf_future.groupby(\"Year\", as_index=False)[\"Yield\"].mean().rename(columns={\"Yield\": \"Yield_pred\"})\n",
        "\n",
        "# Step 2: Historical yield (1980–2020), reuse val_mask and y_train/y_val from earlier RF setup\n",
        "encoded_years = wheat_train[\"tasmax\"][\"year\"]\n",
        "\n",
        "# Decode training and validation years\n",
        "train_years = decode_year(encoded_years[~val_mask].astype(int))\n",
        "val_years = decode_year(encoded_years[val_mask].astype(int))\n",
        "\n",
        "historical_years_all = pd.concat([train_years.reset_index(drop=True), val_years.reset_index(drop=True)])\n",
        "historical_yields_all = np.concatenate([y_train, y_val])\n",
        "\n",
        "historical_df = pd.DataFrame({\n",
        "    \"Year\": historical_years_all,\n",
        "    \"Yield_hist\": historical_yields_all\n",
        "})\n",
        "historical_avg = historical_df.groupby(\"Year\", as_index=False)[\"Yield_hist\"].mean()\n",
        "\n",
        "# Step 3: Merge into full 1980–2100 DataFrame\n",
        "all_years = pd.DataFrame({\"Year\": range(1980, 2101)})\n",
        "historical_avg[\"Year\"] = historical_avg[\"Year\"].astype(int)\n",
        "rf_future_avg[\"Year\"] = rf_future_avg[\"Year\"].astype(int)\n",
        "\n",
        "plot_df = all_years.merge(historical_avg, on=\"Year\", how=\"left\")\n",
        "plot_df = plot_df.merge(rf_future_avg, on=\"Year\", how=\"left\")\n",
        "\n",
        "# Step 4: Plot Random Forest line (same style as XGBoost version)\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_hist\"].notna()],\n",
        "         plot_df[\"Yield_hist\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:blue\", label=\"Historical Yield (1980–2020)\")\n",
        "\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_pred\"].notna()],\n",
        "         plot_df[\"Yield_pred\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:orange\", label=\"Random Forest Predicted Yield (2021–2100)\")\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Random Forest Wheat Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "isLmdAfF2a5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine historical and predicted yields into one continuous series\n",
        "combined_yield = pd.concat([\n",
        "    plot_df.set_index(\"Year\")[\"Yield_hist\"].dropna(),\n",
        "    plot_df.set_index(\"Year\")[\"Yield_pred\"].dropna()\n",
        "])\n",
        "\n",
        "# Plot the full timeline\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(combined_yield.index, combined_yield.values, marker=\"o\", color=\"tab:purple\", label=\"Random Forest: Historical + Predicted Yield\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield (t/ha) across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Random Forest Wheat Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wsGYkH472drh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from XGBoost\n",
        "importances = model.feature_importances_\n",
        "features = numeric_features_wheat.columns\n",
        "sorted_idx = np.argsort(importances)[::-1]  # Descending order\n",
        "top_idx = sorted_idx[:10]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importances[top_idx], y=features[top_idx], ci=None)\n",
        "plt.title(\"Feature Importance from Random Forest(Wheat)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pW4YC-bO2ftE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare historical and predicted yield DataFrames (same structure as XGBoost version)\n",
        "historical_rf_df = historical_df[[\"Year\", \"Yield_hist\"]].rename(columns={\"Yield_hist\": \"Yield\"})\n",
        "historical_rf_df = historical_rf_df[(historical_rf_df[\"Year\"] >= 1980) & (historical_rf_df[\"Year\"] <= 2020)]\n",
        "\n",
        "future_rf_df = rf_pred_df[rf_pred_df[\"Year\"] >= 2021]\n",
        "\n",
        "# KDE plot for yield distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(data=historical_rf_df, x=\"Yield\", label=\"1980–2020\", fill=True)\n",
        "sns.kdeplot(data=future_rf_df, x=\"Yield\", label=\"2021–2100\", fill=True)\n",
        "plt.title(\"Yield Distribution: Historical vs Predicted (Random Forest)\")\n",
        "plt.xlabel(\"Yield (t/ha)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Xv0gY5p2hW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost (Maize)"
      ],
      "metadata": {
        "id": "uTQEcckC2jhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "AjYMDn9-2lUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "sc4WEnPB2qS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "8J4GV8142rp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_future_crop_challenge_path = kagglehub.competition_download('the-future-crop-challenge')\n",
        "print(\"data import successfully\")"
      ],
      "metadata": {
        "id": "Axs9Yh5w2tHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(crop: str, mode: str=\"train\"):\n",
        "    base_path = the_future_crop_challenge_path\n",
        "\n",
        "    tasmax = pd.read_parquet(f\"{base_path}/tasmax_{crop}_{mode}.parquet\")\n",
        "    tasmin = pd.read_parquet(f\"{base_path}/tasmin_{crop}_{mode}.parquet\")\n",
        "    pr = pd.read_parquet(f\"{base_path}/pr_{crop}_{mode}.parquet\")\n",
        "    rsds = pd.read_parquet(f\"{base_path}/rsds_{crop}_{mode}.parquet\")\n",
        "    soil_co2 = pd.read_parquet(f\"{base_path}/soil_co2_{crop}_{mode}.parquet\")\n",
        "\n",
        "    target = None\n",
        "    if mode == \"train\":\n",
        "        target = pd.read_parquet(f\"{base_path}/train_solutions_{crop}.parquet\")\n",
        "\n",
        "    return {\n",
        "        'tasmax': tasmax,\n",
        "        'tasmin': tasmin,\n",
        "        'pr': pr,\n",
        "        'rsds': rsds,\n",
        "        'soil_co2': soil_co2,\n",
        "        'target': target,\n",
        "    }"
      ],
      "metadata": {
        "id": "qaaW3to72uwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maize"
      ],
      "metadata": {
        "id": "6y_OUAL62v_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data for maize\n",
        "maize_train = load_data(\"maize\", \"train\")"
      ],
      "metadata": {
        "id": "q0k788t62xC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_daily_data(df):\n",
        "    numeric_df = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
        "    return numeric_df.agg(['mean', 'max', 'min', 'std'], axis=1)\n",
        "tasmax_agg_maize = aggregate_daily_data(maize_train['tasmax'])\n",
        "tasmin_agg_maize = aggregate_daily_data(maize_train['tasmin'])\n",
        "pr_agg_maize = aggregate_daily_data(maize_train['pr'])\n",
        "rsds_agg_maize = aggregate_daily_data(maize_train['rsds'])\n",
        "tasmax_agg_maize.columns = [f'tasmax_{col}' for col in tasmax_agg_maize.columns]\n",
        "tasmin_agg_maize.columns = [f'tasmin_{col}' for col in tasmin_agg_maize.columns]\n",
        "pr_agg_maize.columns = [f'pr_{col}' for col in pr_agg_maize.columns]\n",
        "rsds_agg_maize.columns = [f'rsds_{col}' for col in rsds_agg_maize.columns]\n",
        "\n",
        "feature_maize = pd.concat([tasmax_agg_maize, tasmin_agg_maize, pr_agg_maize, rsds_agg_maize, maize_train['soil_co2']], axis = 1)\n",
        "target_maize = maize_train['target']"
      ],
      "metadata": {
        "id": "wvXQ7oyD2yOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop non-numeric columns\n",
        "numeric_features_maize = feature_maize.select_dtypes(include=[np.number])\n",
        "\n",
        "# Scale features\n",
        "scaler_maize = StandardScaler()\n",
        "features_scaled_maize = scaler_maize.fit_transform(numeric_features_maize)"
      ],
      "metadata": {
        "id": "tl5HNv-q2zU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the year column\n",
        "year_series = maize_train['tasmax']['year']\n",
        "\n",
        "# Use the maximum year as validation year\n",
        "val_year = year_series.max()\n",
        "print(f\"Using year {val_year} as validation set\") # Note that again this is not printing out the real year of 2020\n",
        "\n",
        "# Create boolean mask for validation\n",
        "val_mask = (year_series == val_year)\n",
        "\n",
        "# Split data\n",
        "X_train = features_scaled_maize[~val_mask]\n",
        "X_val = features_scaled_maize[val_mask]\n",
        "y_train = target_maize[~val_mask].values.ravel()\n",
        "y_val = target_maize[val_mask].values.ravel()"
      ],
      "metadata": {
        "id": "iveEye6C21TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")"
      ],
      "metadata": {
        "id": "x57rtE8s21Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model.fit(X_train, y_train)\n",
        "# Predict\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "# Evaluation\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "print(f\"XGBoost Validation RMSE: {rmse:.4f}\")\n",
        "print(f\"XGBoost Validation R2: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "G2lEdeeM21fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Predicted vs Actual\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=y_val, y=y_pred, alpha=0.6, edgecolor=None)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield\")\n",
        "plt.ylabel(\"Predicted Yield\")\n",
        "plt.title(f\"XGBoost Predicted vs Actual Maize Yield (2020)\\nR² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h5FtBqK93DYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_val - y_pred\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(residuals, bins=30, kde=True, color='green')\n",
        "plt.title(\"XGBoost Residuals (2020): Actual - Predicted (Maize)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ulM7tJkQ3E3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "maize_test = load_data(\"maize\", \"test\")\n",
        "\n",
        "tasmax_agg_test = aggregate_daily_data(maize_test['tasmax'])\n",
        "tasmax_agg_test.columns = [f'tasmax_{col}' for col in tasmax_agg_test.columns]\n",
        "\n",
        "tasmin_agg_test = aggregate_daily_data(maize_test['tasmin'])\n",
        "tasmin_agg_test.columns = [f'tasmin_{col}' for col in tasmin_agg_test.columns]\n",
        "\n",
        "pr_agg_test = aggregate_daily_data(maize_test['pr'])\n",
        "pr_agg_test.columns = [f'pr_{col}' for col in pr_agg_test.columns]\n",
        "\n",
        "rsds_agg_test = aggregate_daily_data(maize_test['rsds'])\n",
        "rsds_agg_test.columns = [f'rsds_{col}' for col in rsds_agg_test.columns]\n",
        "\n",
        "features_test = pd.concat([tasmax_agg_test, tasmin_agg_test, pr_agg_test, rsds_agg_test, maize_test['soil_co2']], axis=1)\n",
        "\n",
        "numeric_features_test = features_test.select_dtypes(include=[np.number])\n",
        "\n",
        "numeric_features_test = numeric_features_test.dropna()\n",
        "\n",
        "features_scaled_test = scaler_maize.transform(numeric_features_test)\n",
        "test_ids = numeric_features_test.index"
      ],
      "metadata": {
        "id": "s7X5wJLe3GHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_predictions = xgb_model.predict(features_scaled_test)\n",
        "\n",
        "# Store predictions with index\n",
        "xgb_results = pd.DataFrame({\n",
        "    'ID': numeric_features_test.index,\n",
        "    'Predicted_Yield': xgb_predictions\n",
        "})\n",
        "\n",
        "xgb_results.head()"
      ],
      "metadata": {
        "id": "bxd-P7LU3IX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Year decoder function\n",
        "def decode_year(encoded_year):\n",
        "    return encoded_year - 381 + 1980\n",
        "\n",
        "# Step 1: XGBoost predictions (2021–2100), with decoded years\n",
        "xgb_pred_df = pd.DataFrame({\n",
        "    \"Year\": decode_year(maize_test['tasmax'].loc[numeric_features_test.index, 'year']).astype(int),\n",
        "    \"Yield\": xgb_predictions\n",
        "}, index=numeric_features_test.index)\n",
        "\n",
        "xgb_future = xgb_pred_df[(xgb_pred_df[\"Year\"] >= 2021) & (xgb_pred_df[\"Year\"] <= 2100)]\n",
        "xgb_future_avg = xgb_future.groupby(\"Year\", as_index=False)[\"Yield\"].mean().rename(columns={\"Yield\": \"Yield_pred\"})\n",
        "\n",
        "# Step 2: Historical yield (1980–2020 including validation), with decoded years\n",
        "encoded_years = maize_train[\"tasmax\"][\"year\"]\n",
        "\n",
        "# Use val_mask to split years for train and val\n",
        "train_years = decode_year(encoded_years[~val_mask].astype(int))\n",
        "val_years = decode_year(encoded_years[val_mask].astype(int))\n",
        "\n",
        "historical_years_all = pd.concat([train_years.reset_index(drop=True), val_years.reset_index(drop=True)])\n",
        "historical_yields_all = np.concatenate([y_train, y_val])\n",
        "\n",
        "historical_df = pd.DataFrame({\n",
        "    \"Year\": historical_years_all,\n",
        "    \"Yield_hist\": historical_yields_all\n",
        "})\n",
        "historical_avg = historical_df.groupby(\"Year\", as_index=False)[\"Yield_hist\"].mean()\n",
        "\n",
        "# Step 3: Combine into full plot DataFrame (1980–2100)\n",
        "all_years = pd.DataFrame({\"Year\": range(1980, 2101)})\n",
        "historical_avg[\"Year\"] = historical_avg[\"Year\"].astype(int)\n",
        "xgb_future_avg[\"Year\"] = xgb_future_avg[\"Year\"].astype(int)\n",
        "\n",
        "plot_df = all_years.merge(historical_avg, on=\"Year\", how=\"left\")\n",
        "plot_df = plot_df.merge(xgb_future_avg, on=\"Year\", how=\"left\")\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_hist\"].notna()],\n",
        "         plot_df[\"Yield_hist\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:blue\", label=\"Historical Yield (1980–2020)\")\n",
        "\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_pred\"].notna()],\n",
        "         plot_df[\"Yield_pred\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:green\", label=\"XGBoost Predicted Yield (2021–2100)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Maize Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tQ6wJpGp3MUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine historical + predicted into one series\n",
        "combined_yield = pd.concat([\n",
        "    plot_df.set_index(\"Year\")[\"Yield_hist\"].dropna(),\n",
        "    plot_df.set_index(\"Year\")[\"Yield_pred\"].dropna()\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(combined_yield.index, combined_yield.values, marker=\"o\", color=\"tab:purple\", label=\"Historical + Predicted Yield\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield (t/ha)\")\n",
        "plt.title(\"Maize Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Gf-4kAz3MXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from XGBoost\n",
        "importances = xgb_model.feature_importances_\n",
        "features = numeric_features_maize.columns\n",
        "sorted_idx = np.argsort(importances)[::-1]  # Descending order\n",
        "top_idx = sorted_idx[:10]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importances[top_idx], y=features[top_idx], ci=None)\n",
        "plt.title(\"Feature Importance from XGBoost(Maize)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c__Uk6vt3MZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = xgb_model.feature_importances_\n",
        "features = numeric_features_maize.columns\n",
        "\n",
        "# Create a DataFrame of features and their importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance descending\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display top N (optional)\n",
        "importance_df.head(20)"
      ],
      "metadata": {
        "id": "v479EHQ33Mb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare historical and predicted yield DataFrames\n",
        "historical_xgb_df = historical_df[[\"Year\", \"Yield_hist\"]].rename(columns={\"Yield_hist\": \"Yield\"})\n",
        "historical_xgb_df = historical_xgb_df[(historical_xgb_df[\"Year\"] >= 1980) & (historical_xgb_df[\"Year\"] <= 2020)]\n",
        "\n",
        "future_xgb_df = xgb_pred_df[xgb_pred_df[\"Year\"] >= 2021]\n",
        "\n",
        "# KDE plot for yield distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(data=historical_xgb_df, x=\"Yield\", label=\"1980–2020\", fill=True)\n",
        "sns.kdeplot(data=future_xgb_df, x=\"Yield\", label=\"2021–2100\", fill=True)\n",
        "plt.title(\"Maize Yield Distribution: Historical vs Predicted (XGBoost)\")\n",
        "plt.xlabel(\"Yield\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4fZaXPZ13UiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost (Wheat)"
      ],
      "metadata": {
        "id": "WVBjGszqzCMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "sHgi_gVizEJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def load_data(crop: str, mode: str=\"train\"):\n",
        "    base_path = \"/kaggle/input/the-future-crop-challenge\"\n",
        "\n",
        "    tasmax = pd.read_parquet(f\"{base_path}/tasmax_{crop}_{mode}.parquet\")\n",
        "    tasmin = pd.read_parquet(f\"{base_path}/tasmin_{crop}_{mode}.parquet\")\n",
        "    pr = pd.read_parquet(f\"{base_path}/pr_{crop}_{mode}.parquet\")\n",
        "    rsds = pd.read_parquet(f\"{base_path}/rsds_{crop}_{mode}.parquet\")\n",
        "    soil_co2 = pd.read_parquet(f\"{base_path}/soil_co2_{crop}_{mode}.parquet\")\n",
        "\n",
        "    target = None\n",
        "    if mode == \"train\":\n",
        "        target = pd.read_parquet(f\"{base_path}/train_solutions_{crop}.parquet\")\n",
        "\n",
        "    return {\n",
        "        'tasmax': tasmax,\n",
        "        'tasmin': tasmin,\n",
        "        'pr': pr,\n",
        "        'rsds': rsds,\n",
        "        'soil_co2': soil_co2,\n",
        "        'target': target,\n",
        "    }\n",
        "\n",
        "wheat_train = load_data(\"wheat\", \"train\")"
      ],
      "metadata": {
        "id": "bBa7R3uYzqfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_train['soil_co2']"
      ],
      "metadata": {
        "id": "E0ek1uH_zsHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_train['tasmax']"
      ],
      "metadata": {
        "id": "PHB_LZmsztNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_daily_data(df):\n",
        "    numeric_df = df.iloc[:, 4:].apply(pd.to_numeric, errors='coerce')\n",
        "    return numeric_df.agg(['mean', 'max', 'min', 'std'], axis=1)\n",
        "\n",
        "tasmax_agg_wheat = aggregate_daily_data(wheat_train['tasmax'])\n",
        "tasmin_agg_wheat = aggregate_daily_data(wheat_train['tasmin'])\n",
        "pr_agg_wheat = aggregate_daily_data(wheat_train['pr'])\n",
        "rsds_agg_wheat = aggregate_daily_data(wheat_train['rsds'])\n",
        "\n",
        "tasmax_agg_wheat.columns = [f'tasmax_{col}' for col in tasmax_agg_wheat.columns]\n",
        "tasmin_agg_wheat.columns = [f'tasmin_{col}' for col in tasmin_agg_wheat.columns]\n",
        "pr_agg_wheat.columns     = [f'pr_{col}' for col in pr_agg_wheat.columns]\n",
        "rsds_agg_wheat.columns   = [f'rsds_{col}' for col in rsds_agg_wheat.columns]\n",
        "\n",
        "features_wheat = pd.concat([tasmax_agg_wheat, tasmin_agg_wheat, pr_agg_wheat, rsds_agg_wheat, wheat_train['soil_co2']], axis=1)\n",
        "target_wheat = wheat_train['target']"
      ],
      "metadata": {
        "id": "vpJLSfrezucm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_wheat"
      ],
      "metadata": {
        "id": "tmhqstG3zxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_wheat['texture_class']"
      ],
      "metadata": {
        "id": "fVLgRE3kzzjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features_wheat.columns.tolist())"
      ],
      "metadata": {
        "id": "HrKgIjQOz09a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features_wheat = features_wheat.select_dtypes(include=[np.number])\n",
        "scaler_wheat = StandardScaler()\n",
        "features_scaled_wheat = scaler_wheat.fit_transform(numeric_features_wheat)"
      ],
      "metadata": {
        "id": "x6I338Fgz2Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_series = wheat_train['tasmax']['year']\n",
        "\n",
        "val_year = year_series.max()\n",
        "print(f\"Using year {val_year} as validation set\") #Note that here again, this is not the actual year of 2020\n",
        "\n",
        "val_mask = (year_series == val_year)\n",
        "\n",
        "# Split data\n",
        "X_train = features_scaled_wheat[~val_mask]\n",
        "X_val = features_scaled_wheat[val_mask]\n",
        "y_train = target_wheat[~val_mask].values.ravel()\n",
        "y_val = target_wheat[val_mask].values.ravel()"
      ],
      "metadata": {
        "id": "IOFx3j0Sz3Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "print(f\"XGBoost Validation RMSE: {rmse:.4f}\")\n",
        "print(f\"XGBoost Validation R2: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "I22dQ157z6Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=y_val, y=y_pred)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield\")\n",
        "plt.ylabel(\"Predicted Yield\")\n",
        "plt.title(f\"XGBoost Predicted vs Actual Wheat Yield (Validation Set)\\nR² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_6BJqLF60GQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_val - y_pred\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(residuals, bins=30, kde=True, color='green')\n",
        "plt.title(\"XGBoost Residual Distribution: Actual - Predicted(Wheat)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pOS2wuW50HeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_test = load_data(\"wheat\", \"test\")\n",
        "\n",
        "tasmax_agg_test = aggregate_daily_data(wheat_test['tasmax'])\n",
        "tasmax_agg_test.columns = [f'tasmax_{col}' for col in tasmax_agg_test.columns]\n",
        "\n",
        "tasmin_agg_test = aggregate_daily_data(wheat_test['tasmin'])\n",
        "tasmin_agg_test.columns = [f'tasmin_{col}' for col in tasmin_agg_test.columns]\n",
        "\n",
        "pr_agg_test = aggregate_daily_data(wheat_test['pr'])\n",
        "pr_agg_test.columns = [f'pr_{col}' for col in pr_agg_test.columns]\n",
        "\n",
        "rsds_agg_test = aggregate_daily_data(wheat_test['rsds'])\n",
        "rsds_agg_test.columns = [f'rsds_{col}' for col in rsds_agg_test.columns]\n",
        "\n",
        "\n",
        "features_test = pd.concat([tasmax_agg_test, tasmin_agg_test, pr_agg_test, rsds_agg_test, wheat_test['soil_co2']], axis=1)\n",
        "\n",
        "numeric_features_test = features_test.select_dtypes(include=[np.number]).dropna()\n",
        "\n",
        "features_scaled_test = scaler_wheat.transform(numeric_features_test)\n",
        "\n",
        "xgb_predictions = xgb_model.predict(features_scaled_test)\n",
        "\n",
        "xgb_results = pd.DataFrame({\n",
        "    'ID': numeric_features_test.index,\n",
        "    'Predicted_Yield': xgb_predictions\n",
        "})\n",
        "\n",
        "xgb_results.head()"
      ],
      "metadata": {
        "id": "-t8OD_zp0IKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Year decoder function\n",
        "def decode_year(encoded_year):\n",
        "    return encoded_year - 381 + 1980\n",
        "\n",
        "# Step 1: XGBoost predictions (2021–2100), with decoded years\n",
        "xgb_pred_df = pd.DataFrame({\n",
        "    \"Year\": decode_year(wheat_test['tasmax'].loc[numeric_features_test.index, 'year']).astype(int),\n",
        "    \"Yield\": xgb_predictions\n",
        "}, index=numeric_features_test.index)\n",
        "\n",
        "xgb_future = xgb_pred_df[(xgb_pred_df[\"Year\"] >= 2021) & (xgb_pred_df[\"Year\"] <= 2100)]\n",
        "xgb_future_avg = xgb_future.groupby(\"Year\", as_index=False)[\"Yield\"].mean().rename(columns={\"Yield\": \"Yield_pred\"})\n",
        "\n",
        "# Step 2: Historical yield (1980–2020 including validation), with decoded years\n",
        "encoded_years = wheat_train[\"tasmax\"][\"year\"]\n",
        "\n",
        "# Use val_mask to split years for train and val\n",
        "train_years = decode_year(encoded_years[~val_mask].astype(int))\n",
        "val_years = decode_year(encoded_years[val_mask].astype(int))\n",
        "\n",
        "historical_years_all = pd.concat([train_years.reset_index(drop=True), val_years.reset_index(drop=True)])\n",
        "historical_yields_all = np.concatenate([y_train, y_val])\n",
        "\n",
        "historical_df = pd.DataFrame({\n",
        "    \"Year\": historical_years_all,\n",
        "    \"Yield_hist\": historical_yields_all\n",
        "})\n",
        "historical_avg = historical_df.groupby(\"Year\", as_index=False)[\"Yield_hist\"].mean()\n",
        "\n",
        "# Step 3: Combine into full plot DataFrame (1980–2100)\n",
        "all_years = pd.DataFrame({\"Year\": range(1980, 2101)})\n",
        "historical_avg[\"Year\"] = historical_avg[\"Year\"].astype(int)\n",
        "xgb_future_avg[\"Year\"] = xgb_future_avg[\"Year\"].astype(int)\n",
        "\n",
        "plot_df = all_years.merge(historical_avg, on=\"Year\", how=\"left\")\n",
        "plot_df = plot_df.merge(xgb_future_avg, on=\"Year\", how=\"left\")\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_hist\"].notna()],\n",
        "         plot_df[\"Yield_hist\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:blue\", label=\"Historical Yield (1980–2020)\")\n",
        "\n",
        "plt.plot(plot_df[\"Year\"][plot_df[\"Yield_pred\"].notna()],\n",
        "         plot_df[\"Yield_pred\"].dropna(),\n",
        "         marker=\"o\", color=\"tab:green\", label=\"XGBoost Predicted Yield (2021–2100)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield across highly-harvested gridcells (>2000 hectares)\")\n",
        "plt.title(\"Wheat Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "11l_EOcz0Jll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine historical + predicted into one series\n",
        "combined_yield = pd.concat([\n",
        "    plot_df.set_index(\"Year\")[\"Yield_hist\"].dropna(),\n",
        "    plot_df.set_index(\"Year\")[\"Yield_pred\"].dropna()\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(combined_yield.index, combined_yield.values, marker=\"o\", color=\"tab:purple\", label=\"Historical + Predicted Yield\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean yield (t/ha)\")\n",
        "plt.title(\"Wheat Yield across 1980–2100\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.xlim(1980, 2100)\n",
        "plt.xticks(range(1980, 2101, 10))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-herl4T0L4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from XGBoost\n",
        "importances = xgb_model.feature_importances_\n",
        "features = numeric_features_wheat.columns\n",
        "sorted_idx = np.argsort(importances)[::-1]\n",
        "top_idx = sorted_idx[:10]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importances[top_idx], y=features[top_idx], ci=None)\n",
        "plt.title(\"Feature Importance from XGBoost(Wheat)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_mdRVsX_0OHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = xgb_model.feature_importances_\n",
        "features = numeric_features_wheat.columns\n",
        "\n",
        "# Create a DataFrame of features and their importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance descending\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display top N (optional)\n",
        "importance_df.head(10)"
      ],
      "metadata": {
        "id": "bHQ0iUbD0Pvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare historical and predicted yield DataFrames\n",
        "historical_xgb_df = historical_df[[\"Year\", \"Yield_hist\"]].rename(columns={\"Yield_hist\": \"Yield\"})\n",
        "historical_xgb_df = historical_xgb_df[(historical_xgb_df[\"Year\"] >= 1980) & (historical_xgb_df[\"Year\"] <= 2020)]\n",
        "\n",
        "future_xgb_df = xgb_pred_df[xgb_pred_df[\"Year\"] >= 2021]\n",
        "\n",
        "# KDE plot for yield distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(data=historical_xgb_df, x=\"Yield\", label=\"1980–2020\", fill=True)\n",
        "sns.kdeplot(data=future_xgb_df, x=\"Yield\", label=\"2021–2100\", fill=True)\n",
        "plt.title(\"Wheat Yield Distribution: Historical vs Predicted (XGBoost)\")\n",
        "plt.xlabel(\"Yield (t/ha)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "toXY_NDM0QXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unidirectional LSTM (Ran on Colab with Google drive)"
      ],
      "metadata": {
        "id": "BvcH_ZLT4KSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from time import time\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split,DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "IDJF05ch4MZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zco7mf7G4POW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Use correct path inside Drive\n",
        "mypath = '/content/drive/MyDrive/the-future-crop-challenge'"
      ],
      "metadata": {
        "id": "qjMSb8Ku4UlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "# DEVICE = torch.device('cuda:0')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "L90tE-Yu4XyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "Z5TmyOjA4cPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_region(row):\n",
        "    if -125 <= row['lon'] <= -66 and 24 <= row['lat'] <= 50:\n",
        "        return 'USA'\n",
        "    elif -75 <= row['lon'] <= -35 and -40 <= row['lat'] <= -10:\n",
        "        return 'South America'\n",
        "    elif -10 <= row['lon'] <= 50 and 35 <= row['lat'] <= 60:\n",
        "        return 'Europe'\n",
        "    else:\n",
        "        return 'Other'"
      ],
      "metadata": {
        "id": "BaM3BsRv4fLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(crop,mode,data_dir):\n",
        "    tasmax = pd.read_parquet(os.path.join(data_dir, \"tasmax_{}_{}.parquet\".format(crop,mode)))\n",
        "    tasmin = pd.read_parquet(os.path.join(data_dir, \"tasmin_{}_{}.parquet\".format(crop,mode)))\n",
        "    tas = pd.read_parquet(os.path.join(data_dir, \"tas_{}_{}.parquet\".format(crop,mode)))\n",
        "    pr = pd.read_parquet(os.path.join(data_dir, \"pr_{}_{}.parquet\".format(crop,mode)))\n",
        "    rsds = pd.read_parquet(os.path.join(data_dir, \"rsds_{}_{}.parquet\".format(crop,mode)))\n",
        "    soil_co2 = pd.read_parquet(os.path.join(data_dir, \"soil_co2_{}_{}.parquet\".format(crop,mode)))\n",
        "\n",
        "    if mode == 'train':\n",
        "        yield_ = pd.read_parquet(os.path.join(data_dir, \"{}_solutions_{}.parquet\".format(mode,crop)))\n",
        "        yield_ = yield_.values.astype(np.float32)\n",
        "\n",
        "    if mode == 'test':\n",
        "        yield_ = None\n",
        "\n",
        "    climate = np.concatenate([\n",
        "        tas.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        tasmax.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        tasmin.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        pr.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        rsds.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "    ], axis=2)\n",
        "    return(climate,yield_,soil_co2)"
      ],
      "metadata": {
        "id": "EQ54AR0v4gYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataloader(crop, mode, mypath, region, static_var, detrend=True):\n",
        "    (climate, yield_label, soil) = load(crop, mode, mypath)\n",
        "\n",
        "    # Reshape and calculate mean and std along grouped axis\n",
        "    climate_mean = climate.reshape(climate.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_sd = climate.reshape(climate.shape[0], 8, 30, 5).std(axis=2)\n",
        "\n",
        "    GPP = np.maximum(climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "    heat_stress_day = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "    frost_days = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "\n",
        "    climate_stats = np.concatenate([climate_sd, climate_mean, GPP, heat_stress_day, frost_days], axis=2)\n",
        "\n",
        "    # Annual mean\n",
        "    annual_df = pd.DataFrame(climate.mean(axis=1), columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil = soil.reset_index(drop=True)\n",
        "    soil = pd.concat([annual_df, soil], axis=1)\n",
        "\n",
        "    soil['region'] = soil.apply(get_region, axis=1)\n",
        "    soil['co2'] = soil['co2'] / 1500\n",
        "\n",
        "    # Detrend yield\n",
        "    soil['yield'] = yield_label\n",
        "    grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "    grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "    soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "    soil['de_yield'] = soil['yield'] - soil['yield_mean']\n",
        "\n",
        "    # Scale input features\n",
        "    climate_arr = climate_stats.reshape(-1, climate_stats.shape[2])\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(climate_arr)\n",
        "    scaled_climate = scaler.transform(climate_arr).reshape(climate_stats.shape)\n",
        "\n",
        "    # Scale static features\n",
        "    scaler_static = StandardScaler()\n",
        "    static_arr = soil[static_var].values.astype(np.float32)\n",
        "    scaler_static.fit(static_arr)\n",
        "    scaled_static = scaler_static.transform(static_arr)\n",
        "    scaled_static = np.concatenate([scaled_static, soil['co2'].values.reshape(-1, 1)], axis=1)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X = torch.from_numpy(scaled_climate).float()\n",
        "    X_static = torch.from_numpy(scaled_static).float()\n",
        "    y = torch.from_numpy(\n",
        "        soil['de_yield'].values.reshape(-1, 1) if detrend else soil['yield'].values.reshape(-1, 1)\n",
        "    ).float()\n",
        "\n",
        "    dataset = TensorDataset(X, X_static, y)\n",
        "\n",
        "    # Filter by region\n",
        "    Region_index = soil[soil['region'].isin(region)].index.values\n",
        "    dataset = torch.utils.data.Subset(dataset, Region_index)\n",
        "    soil = soil.loc[Region_index].reset_index(drop=True)  # Align index with Subset\n",
        "\n",
        "    print(f\"After region filtering: {len(dataset)} samples\")\n",
        "\n",
        "    # 🟨 Year-based split using real_year\n",
        "    if 'real_year' not in soil.columns:\n",
        "        raise ValueError(\"'real_year' column not found in soil dataframe.\")\n",
        "\n",
        "    val_indices = soil[soil['real_year'] == 2020].index.tolist()\n",
        "    train_indices = soil[soil['real_year'] != 2020].index.tolist()\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    # Optional: take last 10% of train as test set\n",
        "    test_split = int(0.1 * len(train_dataset))\n",
        "    test_dataset = torch.utils.data.Subset(train_dataset, list(range(len(train_dataset) - test_split, len(train_dataset))))\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, list(range(0, len(train_dataset) - test_split)))\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3000, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=3000, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3000, shuffle=False)\n",
        "\n",
        "    print(f\"Train: {len(train_loader.dataset)}, Val (2020): {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "NMXacjKd4hmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "A_9RVPB34jwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMForecaster_static(nn.Module):\n",
        "  def __init__(self, n_features, n_hidden, n_outputs,n_static, n_static_hidden, sequence_len,DEVICE, n_lstm_layers=1, n_deep_layers=10, use_cuda=True, dropout=0.2):\n",
        "    '''\n",
        "    n_features: number of input features (1 for univariate forecasting)\n",
        "    n_hidden: number of neurons in each hidden layer\n",
        "    n_outputs: number of outputs to predict for each training example\n",
        "    n_static: number of static features\n",
        "    n_deep_layers: number of hidden dense layers after the lstm layer\n",
        "    sequence_len: number of steps to look back at for prediction\n",
        "    dropout: float (0 < dropout < 1) dropout ratio between dense layers\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.n_lstm_layers = n_lstm_layers\n",
        "    self.nhid = n_hidden\n",
        "    self.use_cuda = use_cuda # set option for device selection\n",
        "    self.DEVICE = DEVICE # set option for device selection\n",
        "\n",
        "    # LSTM Layer\n",
        "    self.lstm = nn.LSTM(n_features,\n",
        "                        n_hidden,\n",
        "                        num_layers=n_lstm_layers,\n",
        "                        batch_first=True) # As we have transformed our data in this way\n",
        "\n",
        "\n",
        "    # First dense layer to expand static variables\n",
        "    self.fc1 = nn.Linear(n_static,n_static_hidden)\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    # Create fully connected layers (n_hidden x n_deep_layers)\n",
        "    dnn_layers = []\n",
        "\n",
        "    # the first layer to concatenate hidden and static properties, 2 additional elements\n",
        "    dnn_layers.append(nn.ReLU())\n",
        "    dnn_layers.append(nn.Linear(n_hidden*n_lstm_layers+n_static_hidden, n_hidden))\n",
        "\n",
        "    for i in range(n_deep_layers):\n",
        "      # Last layer (n_hidden x n_outputs)\n",
        "      if i == n_deep_layers - 1:\n",
        "        dnn_layers.append(nn.ReLU())\n",
        "        dnn_layers.append(nn.Linear(n_hidden, n_outputs))\n",
        "      # All other layers (n_hidden x n_hidden) with dropout option\n",
        "      else:\n",
        "        dnn_layers.append(nn.ReLU())\n",
        "        dnn_layers.append(nn.Linear(n_hidden, n_hidden))\n",
        "        if dropout:\n",
        "          dnn_layers.append(nn.Dropout(p=dropout))\n",
        "    # compile DNN layers\n",
        "    self.dnn = nn.Sequential(*dnn_layers)\n",
        "\n",
        "  def forward(self, x,x_static):\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n",
        "    cell_state = torch.zeros(self.n_lstm_layers, x.shape[0], self.nhid)\n",
        "\n",
        "    # move hidden state to device\n",
        "    if self.use_cuda:\n",
        "      hidden_state = hidden_state.to(self.DEVICE)\n",
        "      cell_state = cell_state.to(self.DEVICE)\n",
        "\n",
        "    self.hidden = (hidden_state, cell_state)\n",
        "    # Forward Pass\n",
        "    output, (h,c) = self.lstm(x, self.hidden) # LSTM\n",
        "\n",
        "    # Flatten hidden state of the last step\n",
        "    x_ = self.dropout(h.contiguous().view(x.shape[0], -1))\n",
        "    x_static = self.relu1(self.fc1(x_static))\n",
        "    # Pass forward hidden state and static features through fully connected DNN.\n",
        "    return self.dnn(torch.cat((x_,x_static),axis=1)).squeeze()"
      ],
      "metadata": {
        "id": "AcacrAhT4lKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, test_loader,static_var,DEVICE,lr, weight_decay,step_size,sch_gamma,num_epochs=100):\n",
        "    # Initialize the loss function and optimizer\n",
        "    criterion = nn.MSELoss().to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=sch_gamma)\n",
        "    #static & scaled\n",
        "    # num_epochs = 100\n",
        "    train_losses = [] #save performance metrics\n",
        "    train_r2s = []\n",
        "    val_losses = []\n",
        "    val_r2s = []\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time()\n",
        "        model.train()\n",
        "        for i, (x, x_static, y_true), in enumerate(train_loader):\n",
        "            # model.train()\n",
        "            # print(i)\n",
        "            xt,x_st, yt = x.to(DEVICE),x_static.to(DEVICE),y_true.to(DEVICE).squeeze()\n",
        "\n",
        "            y_pred = model(xt,x_st)\n",
        "            loss_batch = criterion(y_pred, yt)\n",
        "            optimizer.zero_grad()\n",
        "            loss_batch.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 10 != 0:\n",
        "            continue\n",
        "        train_loss,train_r2 = test_mse_r2_static(model, criterion, train_loader,DEVICE)\n",
        "        val_loss,val_r2 = test_mse_r2_static(model, criterion, val_loader,DEVICE)\n",
        "        test_loss,test_r2 = test_mse_r2_static(model, criterion, test_loader,DEVICE)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_r2s.append(train_r2)\n",
        "        val_losses.append(val_loss)\n",
        "        val_r2s.append(val_r2)\n",
        "        end = time()\n",
        "\n",
        "        print(\"Epoch %d: TRAIN MSE loss: %.6f, TRAIN r2 loss: %.6f\" % (epoch, train_loss,train_r2))\n",
        "        print(\"Epoch %d: VAL MSE loss: %.6f, VAL r2: %.6f\" % (epoch, val_loss, val_r2))\n",
        "        print(\"Epoch %d: TEST MSE loss: %.6f, TEST r2: %.6f\" % (epoch, test_loss, test_r2))\n",
        "        print(\"Finish with for one epoch:{} second\".format(end - start))\n",
        "    return model"
      ],
      "metadata": {
        "id": "XBaGEzOV4mUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r2_loss(output, target):\n",
        "    target_mean = torch.mean(target)\n",
        "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
        "    ss_res = torch.sum((target - output) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ],
      "metadata": {
        "id": "WMdNtsKU4nub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mse_r2_static(model,loss_fn, dataloader,DEVICE):\n",
        "    model.eval()\n",
        "    loss_test_cum = 0\n",
        "    y_test_pred = []\n",
        "    y_test_label = []\n",
        "    with torch.no_grad():\n",
        "        for i, (x, z, y) in enumerate(dataloader):\n",
        "            y_pred = model(x.to(DEVICE),z.to(DEVICE))\n",
        "            # predicted = model(input)\n",
        "            loss   = loss_fn(y_pred, y.to(DEVICE).squeeze())\n",
        "            loss_test_cum += loss\n",
        "            y_test_pred.append(y_pred)\n",
        "            y_test_label.append(y.to(DEVICE).squeeze())\n",
        "        r2 = r2_loss(torch.cat(y_test_pred),torch.cat(y_test_label))\n",
        "    return loss_test_cum/(i+1),r2"
      ],
      "metadata": {
        "id": "hVsExK6I4pIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (climate, yield_label, soil) = load('maize','train',mypath)\n",
        "static_var=['nitrogen','texture_class','lon','lat','yield_mean','tas', 'tasmax', 'tasmin', 'pr', 'rsds']"
      ],
      "metadata": {
        "id": "Gwoudo0d4qLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maize"
      ],
      "metadata": {
        "id": "rsASI6wa4sQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## make maize train_loader\n",
        "train_loader,val_loader,test_loader = get_training_dataloader('maize','train',mypath,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "gNmXpbYK4uLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x,z,y) = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "XsDsv-Cj4wNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global model training\n",
        "model = LSTMForecaster_static(n_features = x.shape[2], n_hidden = 200, n_outputs = 1,\n",
        "                              n_static = len(static_var)+1,n_static_hidden=200,sequence_len = x.shape[1],\n",
        "                              DEVICE = DEVICE, n_deep_layers=4, use_cuda=USE_CUDA, dropout=0.4).to(DEVICE)\n",
        "\n",
        "maize_unscale_model_GLOB = train_model(model, train_loader,val_loader,test_loader,static_var,DEVICE,\n",
        "                                  lr=4e-4, weight_decay=0.1,step_size=100,sch_gamma=0.8,num_epochs=200)"
      ],
      "metadata": {
        "id": "yugYlZnk4x3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wheat"
      ],
      "metadata": {
        "id": "TayZXKB04zc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## make wheat train_loader\n",
        "train_loader,val_loader,test_loader = get_training_dataloader('wheat','train',mypath,\n",
        "                                                              region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "WX0p9vHo41Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x,z,y) = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "x4i_h9aJ42pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # global model training\n",
        "model = LSTMForecaster_static(\n",
        "    n_features = x.shape[2],\n",
        "    n_hidden = 128,\n",
        "    n_outputs = 1,\n",
        "    n_static = len(static_var) + 1,\n",
        "    n_static_hidden = 128,\n",
        "    sequence_len = x.shape[1],\n",
        "    DEVICE = DEVICE,\n",
        "    n_lstm_layers = 1,\n",
        "    n_deep_layers = 3,\n",
        "    use_cuda = USE_CUDA,\n",
        "    dropout = 0.4\n",
        ").to(DEVICE)\n",
        "\n",
        "wheat_unscale_model_GLOB = train_model(model, train_loader,val_loader,test_loader,static_var,DEVICE,\n",
        "                                   lr=4e-4, weight_decay=0.01,step_size=50,sch_gamma=0.8,num_epochs=200)"
      ],
      "metadata": {
        "id": "BdRDeiVM44Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evalutation"
      ],
      "metadata": {
        "id": "bdhptFHn5DPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_dataloader(climate,climate_test,soil,soil_test,region,static_var):\n",
        "    # Reshape and calculate mean and sd along the grouped axis\n",
        "    climate_test_mean = climate_test.reshape(climate_test.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_test_sd = climate_test.reshape(climate_test.shape[0], 8, 30, 5).std(axis=2)\n",
        "    GPP_test = np.maximum(climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2,keepdims=True).cumsum(axis=1) # cumulative GDD by phases\n",
        "    heat_stress_day_test =  (climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    frost_days_test = (climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    climate_test_stats = np.concatenate([climate_test_sd,climate_test_mean,GPP_test,heat_stress_day_test,frost_days_test],axis=2)\n",
        "\n",
        "    climate_mean = climate.reshape(climate.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_sd = climate.reshape(climate.shape[0], 8, 30, 5).std(axis=2)\n",
        "    GPP = np.maximum(climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2,keepdims=True).cumsum(axis=1) # cumulative GDD by phases\n",
        "    heat_stress_day =  (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    frost_days = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    climate_stats = np.concatenate([climate_sd,climate_mean,GPP,heat_stress_day,frost_days],axis=2)\n",
        "\n",
        "    #annual mean for test\n",
        "    annual_test_df = pd.DataFrame(climate_test.mean(axis=1),columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil_test = pd.concat([annual_test_df, soil_test], axis=1)\n",
        "\n",
        "    #annual mean for train\n",
        "    annual_df = pd.DataFrame(climate.mean(axis=1),columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil = pd.concat([annual_df, soil], axis=1)\n",
        "\n",
        "    #scale input features\n",
        "    climate_arr = climate_stats.reshape(-1, climate_stats.shape[2])\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(climate_arr)\n",
        "    # scaled_climate = scaler.transform(climate_arr).reshape(climate_stats.shape)\n",
        "\n",
        "    #scale static features\n",
        "    scaler_static = StandardScaler()\n",
        "    static_arr = soil[static_var].values.astype(np.float32)\n",
        "    scaler_static.fit(static_arr)\n",
        "    # scaler_static.fit(static_arr)\n",
        "\n",
        "    climate_test_arr = climate_test_stats.reshape(-1, climate_test_stats.shape[2])\n",
        "    climate_test_scale = scaler.transform(climate_test_arr).reshape(climate_test_stats.shape)\n",
        "\n",
        "    static_test_arr = soil_test[static_var].values.astype(np.float32)\n",
        "    static_test = scaler_static.transform(static_test_arr)\n",
        "    #add co2\n",
        "    soil_test['co2'] = soil_test['co2']/1500\n",
        "    static_test = np.concatenate([static_test,soil_test['co2'].values.reshape(-1, 1)],axis = 1)\n",
        "\n",
        "    X_test_to_loader = torch.from_numpy(climate_test_scale).type(torch.FloatTensor)\n",
        "    static_test_loader = torch.from_numpy(static_test).type(torch.FloatTensor)\n",
        "\n",
        "    dataset = TensorDataset(X_test_to_loader, static_test_loader)\n",
        "\n",
        "    # subset by REGION\n",
        "    Region_index = soil_test[soil_test['region'].isin(region)].index.values\n",
        "    dataset = torch.utils.data.Subset(dataset, Region_index)\n",
        "    print(len(dataset))\n",
        "\n",
        "    test_loader_stats = torch.utils.data.DataLoader(dataset, 2000, shuffle=False)\n",
        "\n",
        "    return soil_test, Region_index, test_loader_stats"
      ],
      "metadata": {
        "id": "3PjY20i95B4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make maize test_loader\n",
        "crop = 'maize'\n",
        "(climate, yield_label, soil) = load(crop,'train', mypath)\n",
        "(climate_test, yield_, soil_test_maize) = load(crop,'test', mypath)"
      ],
      "metadata": {
        "id": "UL8u7kBP5GGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detrend yield\n",
        "soil = soil.reset_index()\n",
        "soil['yield'] = yield_label\n",
        "grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil_test_maize = soil_test_maize.reset_index()\n",
        "soil_test_maize = pd.merge(soil_test_maize, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil['de_yield'] = soil['yield']-soil['yield_mean']\n",
        "soil['region'] = soil.apply(get_region, axis=1)\n",
        "soil_test_maize['region'] = soil_test_maize.apply(get_region, axis=1)"
      ],
      "metadata": {
        "id": "M_fd862b5JUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_soil, Region_index_US, test_loader_US = get_test_dataloader(climate,climate_test,soil,soil_test_maize,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "tU9KUQhD5Kuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maize_pred_stats_test = []\n",
        "for i, (x,z) in enumerate(test_loader_US):\n",
        "    with torch.no_grad():\n",
        "        # y_pred = maize_unscale_model_US(x.to(DEVICE),z.to(DEVICE))\n",
        "        y_pred = maize_unscale_model_GLOB(x.to(DEVICE),z.to(DEVICE))\n",
        "        maize_pred_stats_test.append(y_pred.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "bKUPBOvv5MxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make wheat test_loader\n",
        "crop = 'wheat'\n",
        "(climate, yield_label, soil) = load(crop,'train', mypath)\n",
        "(climate_test, yield_, soil_test_wheat) = load(crop,'test', mypath)"
      ],
      "metadata": {
        "id": "JX2H54oi5NHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detrend yield\n",
        "soil = soil.reset_index()\n",
        "soil['yield'] = yield_label\n",
        "grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil_test_wheat = soil_test_wheat.reset_index()\n",
        "soil_test_wheat = pd.merge(soil_test_wheat, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil['de_yield'] = soil['yield']-soil['yield_mean']\n",
        "soil['region'] = soil.apply(get_region, axis=1)\n",
        "soil_test_wheat['region'] = soil_test_wheat.apply(get_region, axis=1)"
      ],
      "metadata": {
        "id": "E1Oj03LT5OIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_soil, Region_index_US, test_loader_US = get_test_dataloader(climate,climate_test,soil,soil_test_wheat,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "OxOayN625Pac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_pred_stats_test = []\n",
        "for i, (x,z) in enumerate(test_loader_US):\n",
        "    with torch.no_grad():\n",
        "        # y_pred = maize_unscale_model_US(x.to(DEVICE),z.to(DEVICE))\n",
        "        y_pred = wheat_unscale_model_GLOB(x.to(DEVICE),z.to(DEVICE))\n",
        "        wheat_pred_stats_test.append(y_pred.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "NwNGzFdG5RPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We skipped the submission part as that was an attemp for Kaggle submission"
      ],
      "metadata": {
        "id": "mJrOrlna5Ue7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(soil_test_maize['preds_yield'], bins=50, alpha=0.6, label='Maize')\n",
        "plt.hist(soil_test_wheat['preds_yield'], bins=50, alpha=0.6, label='Wheat')\n",
        "plt.xlabel('Predicted Yield')\n",
        "plt.ylabel('Number of Grid Cells')\n",
        "plt.title('Distribution of Predicted Yields (Test Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z3ppRxs85TYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(soil_test_maize['lon'], soil_test_maize['lat'],\n",
        "            c=soil_test_maize['preds_yield'],  s=20, alpha=0.7)\n",
        "plt.colorbar(label='Predicted Maize Yield')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Spatial Distribution of Predicted Maize Yields')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hVu_arRP5-vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "static_var=['nitrogen','texture_class','lon','lat','yield_mean','tas', 'tasmax', 'tasmin', 'pr', 'rsds']\n",
        "\n",
        "# For Maize\n",
        "_, _, maize_test_loader = get_training_dataloader(\n",
        "    'maize', 'train', mypath,\n",
        "    region=['USA', 'Other', 'South America', 'Europe'],\n",
        "    static_var=static_var\n",
        ")\n",
        "\n",
        "# For Wheat\n",
        "_, _, wheat_test_loader = get_training_dataloader(\n",
        "    'wheat', 'train', mypath,\n",
        "    region=['USA', 'Other', 'South America', 'Europe'],\n",
        "    static_var=static_var\n",
        ")"
      ],
      "metadata": {
        "id": "yTvrzrh_6AHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def evaluate_lstm(model, test_loader, title_prefix=\"\"):\n",
        "    model.eval()\n",
        "    y_true_all, y_pred_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, x_static, y_true in test_loader:\n",
        "            y_pred = model(x.to(DEVICE), x_static.to(DEVICE)).cpu().numpy()\n",
        "            y_true = y_true.cpu().numpy()\n",
        "            y_true_all.append(y_true)\n",
        "            y_pred_all.append(y_pred)\n",
        "\n",
        "    y_true_all = np.concatenate(y_true_all)\n",
        "    y_pred_all = np.concatenate(y_pred_all)\n",
        "    residuals = y_true_all - y_pred_all\n",
        "    r2 = r2_score(y_true_all, y_pred_all)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
        "\n",
        "    return y_true_all, y_pred_all, residuals, r2, rmse\n",
        "\n",
        "# --- Evaluate both models on 2020 validation set ---\n",
        "y_true_wheat, y_pred_wheat, resid_wheat, r2_wheat, rmse_wheat = evaluate_lstm(wheat_unscale_model_GLOB, val_loader, \"Wheat (2020)\")\n",
        "y_true_maize, y_pred_maize, resid_maize, r2_maize, rmse_maize = evaluate_lstm(maize_unscale_model_GLOB, val_loader, \"Maize (2020)\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "# 1. Wheat: Pred vs Actual\n",
        "axs[0, 0].scatter(y_true_wheat, y_pred_wheat, alpha=0.5, s=15, edgecolor='k')\n",
        "axs[0, 0].plot([y_true_wheat.min(), y_true_wheat.max()],\n",
        "               [y_true_wheat.min(), y_true_wheat.max()],\n",
        "               'r--', label='Perfect Prediction')\n",
        "axs[0, 0].set_title(f'Validation (2020) - Wheat\\nR² = {r2_wheat:.4f}, RMSE = {rmse_wheat:.3f}')\n",
        "axs[0, 0].set_xlabel(\"Actual Yield\")\n",
        "axs[0, 0].set_ylabel(\"Predicted Yield\")\n",
        "axs[0, 0].legend()\n",
        "axs[0, 0].grid(True)\n",
        "\n",
        "# 2. Maize: Pred vs Actual\n",
        "axs[0, 1].scatter(y_true_maize, y_pred_maize, alpha=0.5, s=15, edgecolor='k')\n",
        "axs[0, 1].plot([y_true_maize.min(), y_true_maize.max()],\n",
        "               [y_true_maize.min(), y_true_maize.max()],\n",
        "               'r--', label='Perfect Prediction')\n",
        "axs[0, 1].set_title(f'Validation (2020) - Maize\\nR² = {r2_maize:.4f}, RMSE = {rmse_maize:.3f}')\n",
        "axs[0, 1].set_xlabel(\"Actual Yield\")\n",
        "axs[0, 1].set_ylabel(\"Predicted Yield\")\n",
        "axs[0, 1].legend()\n",
        "axs[0, 1].grid(True)\n",
        "\n",
        "# 3. Residuals Histogram - Wheat\n",
        "resid_w = resid_wheat.ravel()\n",
        "mean_w, std_w = np.mean(resid_w), np.std(resid_w)\n",
        "x_w = np.linspace(resid_w.min(), resid_w.max(), 500)\n",
        "pdf_w = (1 / (std_w * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_w - mean_w) / std_w) ** 2)\n",
        "axs[1, 0].hist(resid_w, bins=50, color='forestgreen', edgecolor='black', alpha=0.7, density=True)\n",
        "axs[1, 0].plot(x_w, pdf_w, 'k--')\n",
        "axs[1, 0].set_title('Residuals Histogram (Wheat - 2020)')\n",
        "axs[1, 0].set_xlabel(\"Residual\")\n",
        "axs[1, 0].set_ylabel(\"Density\")\n",
        "axs[1, 0].grid(True)\n",
        "\n",
        "# 4. Residuals Histogram - Maize\n",
        "resid_m = resid_maize.ravel()\n",
        "mean_m, std_m = np.mean(resid_m), np.std(resid_m)\n",
        "x_m = np.linspace(resid_m.min(), resid_m.max(), 500)\n",
        "pdf_m = (1 / (std_m * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_m - mean_m) / std_m) ** 2)\n",
        "axs[1, 1].hist(resid_m, bins=50, color='darkgreen', edgecolor='black', alpha=0.7, density=True)\n",
        "axs[1, 1].plot(x_m, pdf_m, 'k--')\n",
        "axs[1, 1].set_title('Residuals Histogram (Maize - 2020)')\n",
        "axs[1, 1].set_xlabel(\"Residual\")\n",
        "axs[1, 1].set_ylabel(\"Density\")\n",
        "axs[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sO1xPu-86B96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTM"
      ],
      "metadata": {
        "id": "HToyYyEX6ZzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from time import time\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split,DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "nG7h4Mw06brw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pQ2VTwE96dmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mypath = \"/content/drive/MyDrive/the-future-crop-challenge\""
      ],
      "metadata": {
        "id": "c0CrqLL39sK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "# DEVICE = torch.device('cuda:0')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hxEBqgfF9tW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_region(row):\n",
        "    if -125 <= row['lon'] <= -66 and 24 <= row['lat'] <= 50:\n",
        "        return 'USA'\n",
        "    elif -75 <= row['lon'] <= -35 and -40 <= row['lat'] <= -10:\n",
        "        return 'South America'\n",
        "    elif -10 <= row['lon'] <= 50 and 35 <= row['lat'] <= 60:\n",
        "        return 'Europe'\n",
        "    else:\n",
        "        return 'Other'"
      ],
      "metadata": {
        "id": "CQN8PvL99uEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(crop,mode,data_dir):\n",
        "    tasmax = pd.read_parquet(os.path.join(data_dir, \"tasmax_{}_{}.parquet\".format(crop,mode)))\n",
        "    tasmin = pd.read_parquet(os.path.join(data_dir, \"tasmin_{}_{}.parquet\".format(crop,mode)))\n",
        "    tas = pd.read_parquet(os.path.join(data_dir, \"tas_{}_{}.parquet\".format(crop,mode)))\n",
        "    pr = pd.read_parquet(os.path.join(data_dir, \"pr_{}_{}.parquet\".format(crop,mode)))\n",
        "    rsds = pd.read_parquet(os.path.join(data_dir, \"rsds_{}_{}.parquet\".format(crop,mode)))\n",
        "    soil_co2 = pd.read_parquet(os.path.join(data_dir, \"soil_co2_{}_{}.parquet\".format(crop,mode)))\n",
        "\n",
        "    if mode == 'train':\n",
        "        yield_ = pd.read_parquet(os.path.join(data_dir, \"{}_solutions_{}.parquet\".format(mode,crop)))\n",
        "        yield_ = yield_.values.astype(np.float32)\n",
        "\n",
        "    if mode == 'test':\n",
        "        yield_ = None\n",
        "\n",
        "    climate = np.concatenate([\n",
        "        tas.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        tasmax.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        tasmin.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        pr.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "        rsds.values[:, 5:,np.newaxis].astype(np.float32),\n",
        "    ], axis=2)\n",
        "    return(climate,yield_,soil_co2)"
      ],
      "metadata": {
        "id": "QWJfm4Dt9vAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataloader(crop, mode, mypath, region, static_var, detrend=True):\n",
        "    (climate, yield_label, soil) = load(crop, mode, mypath)\n",
        "\n",
        "    # Reshape and calculate mean and std along grouped axis\n",
        "    climate_mean = climate.reshape(climate.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_sd = climate.reshape(climate.shape[0], 8, 30, 5).std(axis=2)\n",
        "\n",
        "    GPP = np.maximum(climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "    heat_stress_day = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "    frost_days = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2, keepdims=True).cumsum(axis=1)\n",
        "\n",
        "    climate_stats = np.concatenate([climate_sd, climate_mean, GPP, heat_stress_day, frost_days], axis=2)\n",
        "\n",
        "    # Annual mean\n",
        "    annual_df = pd.DataFrame(climate.mean(axis=1), columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil = soil.reset_index(drop=True)\n",
        "    soil = pd.concat([annual_df, soil], axis=1)\n",
        "\n",
        "    soil['region'] = soil.apply(get_region, axis=1)\n",
        "    soil['co2'] = soil['co2'] / 1500\n",
        "\n",
        "    # Detrend yield\n",
        "    soil['yield'] = yield_label\n",
        "    grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "    grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "    soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "    soil['de_yield'] = soil['yield'] - soil['yield_mean']\n",
        "\n",
        "    # Scale input features\n",
        "    climate_arr = climate_stats.reshape(-1, climate_stats.shape[2])\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(climate_arr)\n",
        "    scaled_climate = scaler.transform(climate_arr).reshape(climate_stats.shape)\n",
        "\n",
        "    # Scale static features\n",
        "    scaler_static = StandardScaler()\n",
        "    static_arr = soil[static_var].values.astype(np.float32)\n",
        "    scaler_static.fit(static_arr)\n",
        "    scaled_static = scaler_static.transform(static_arr)\n",
        "    scaled_static = np.concatenate([scaled_static, soil['co2'].values.reshape(-1, 1)], axis=1)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X = torch.from_numpy(scaled_climate).float()\n",
        "    X_static = torch.from_numpy(scaled_static).float()\n",
        "    y = torch.from_numpy(\n",
        "        soil['de_yield'].values.reshape(-1, 1) if detrend else soil['yield'].values.reshape(-1, 1)\n",
        "    ).float()\n",
        "\n",
        "    dataset = TensorDataset(X, X_static, y)\n",
        "\n",
        "    # Filter by region\n",
        "    Region_index = soil[soil['region'].isin(region)].index.values\n",
        "    dataset = torch.utils.data.Subset(dataset, Region_index)\n",
        "    soil = soil.loc[Region_index].reset_index(drop=True)  # Align index with Subset\n",
        "\n",
        "    print(f\"After region filtering: {len(dataset)} samples\")\n",
        "\n",
        "    # 🟨 Year-based split using real_year\n",
        "    if 'real_year' not in soil.columns:\n",
        "        raise ValueError(\"'real_year' column not found in soil dataframe.\")\n",
        "\n",
        "    val_indices = soil[soil['real_year'] == 2020].index.tolist()\n",
        "    train_indices = soil[soil['real_year'] != 2020].index.tolist()\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    # Optional: take last 10% of train as test set\n",
        "    test_split = int(0.1 * len(train_dataset))\n",
        "    test_dataset = torch.utils.data.Subset(train_dataset, list(range(len(train_dataset) - test_split, len(train_dataset))))\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, list(range(0, len(train_dataset) - test_split)))\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3000, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=3000, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3000, shuffle=False)\n",
        "\n",
        "    print(f\"Train: {len(train_loader.dataset)}, Val (2020): {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "-u1t-eg19wat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated BiLSTMForecaster_static using custom BiLSTM from notebook.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- Custom LSTM definitions ----\n",
        "class CustomLSTM(nn.Module):\n",
        "    class _LSTM(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "            super().__init__()\n",
        "            self.hidden_size = hidden_size\n",
        "            self.xh2gate = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
        "\n",
        "        def forward(self, x_t, h_t, c_t):\n",
        "            combined = torch.cat((x_t, h_t), dim=1)\n",
        "            gates = self.xh2gate(combined)\n",
        "            i, f, o, g = gates.chunk(4, dim=1)\n",
        "            i = torch.sigmoid(i)\n",
        "            f = torch.sigmoid(f)\n",
        "            o = torch.sigmoid(o)\n",
        "            g = torch.tanh(g)\n",
        "            c_t = f * c_t + i * g\n",
        "            h_t = o * torch.tanh(c_t)\n",
        "            return h_t, c_t\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            self._LSTM(input_size if i == 0 else hidden_size, hidden_size)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
        "        c = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :]\n",
        "            for layer in range(self.num_layers):\n",
        "                h[layer], c[layer] = self.layers[layer](input_t, h[layer], c[layer])\n",
        "                input_t = h[layer]\n",
        "        return h[-1]\n",
        "\n",
        "class CustomBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.forward_lstm = CustomLSTM(input_size, hidden_size, num_layers)\n",
        "        self.backward_lstm = CustomLSTM(input_size, hidden_size, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_fwd = self.forward_lstm(x)\n",
        "        x_rev = torch.flip(x, dims=[1])\n",
        "        out_bwd = self.backward_lstm(x_rev)\n",
        "        return torch.cat([out_fwd, out_bwd], dim=1)\n",
        "\n",
        "# ---- Modified Forecaster using Custom BiLSTM ----\n",
        "class LSTMForecaster_static(nn.Module):\n",
        "    def __init__(self, n_features, n_hidden, n_outputs, n_static, n_static_hidden,\n",
        "                 sequence_len, DEVICE, n_lstm_layers=1, n_deep_layers=10, use_cuda=True, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.nhid = n_hidden\n",
        "        self.DEVICE = DEVICE\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        self.bilstm = CustomBiLSTM(n_features, n_hidden, n_lstm_layers)\n",
        "\n",
        "        self.fc1 = nn.Linear(n_static, n_static_hidden)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        dnn_layers = [nn.ReLU(),\n",
        "                      nn.Linear(2 * n_hidden + n_static_hidden, n_hidden)]\n",
        "        for i in range(n_deep_layers):\n",
        "            dnn_layers.append(nn.ReLU())\n",
        "            if i == n_deep_layers - 1:\n",
        "                dnn_layers.append(nn.Linear(n_hidden, n_outputs))\n",
        "            else:\n",
        "                dnn_layers.append(nn.Linear(n_hidden, n_hidden))\n",
        "                dnn_layers.append(nn.Dropout(p=dropout))\n",
        "\n",
        "        self.dnn = nn.Sequential(*dnn_layers)\n",
        "\n",
        "    def forward(self, x, x_static):\n",
        "        bilstm_out = self.dropout(self.bilstm(x))\n",
        "        x_static = self.relu1(self.fc1(x_static))\n",
        "        return self.dnn(torch.cat((bilstm_out, x_static), dim=1)).squeeze()"
      ],
      "metadata": {
        "id": "4KIXw0VT9yUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, test_loader,static_var,DEVICE,lr, weight_decay,step_size,sch_gamma,num_epochs=100):\n",
        "    # Initialize the loss function and optimizer\n",
        "    criterion = nn.MSELoss().to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=sch_gamma)\n",
        "    #static & scaled\n",
        "    # num_epochs = 100\n",
        "    train_losses = [] #save performance metrics\n",
        "    train_r2s = []\n",
        "    val_losses = []\n",
        "    val_r2s = []\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time()\n",
        "        model.train()\n",
        "        for i, (x, x_static, y_true), in enumerate(train_loader):\n",
        "            # model.train()\n",
        "            # print(i)\n",
        "            xt,x_st, yt = x.to(DEVICE),x_static.to(DEVICE),y_true.to(DEVICE).squeeze()\n",
        "\n",
        "            y_pred = model(xt,x_st)\n",
        "            loss_batch = criterion(y_pred, yt)\n",
        "            optimizer.zero_grad()\n",
        "            loss_batch.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 10 != 0:\n",
        "            continue\n",
        "        train_loss,train_r2 = test_mse_r2_static(model, criterion, train_loader,DEVICE)\n",
        "        val_loss,val_r2 = test_mse_r2_static(model, criterion, val_loader,DEVICE)\n",
        "        test_loss,test_r2 = test_mse_r2_static(model, criterion, test_loader,DEVICE)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_r2s.append(train_r2)\n",
        "        val_losses.append(val_loss)\n",
        "        val_r2s.append(val_r2)\n",
        "        end = time()\n",
        "\n",
        "        print(\"Epoch %d: TRAIN MSE loss: %.6f, TRAIN r2 loss: %.6f\" % (epoch, train_loss,train_r2))\n",
        "        print(\"Epoch %d: VAL MSE loss: %.6f, VAL r2: %.6f\" % (epoch, val_loss, val_r2))\n",
        "        print(\"Epoch %d: TEST MSE loss: %.6f, TEST r2: %.6f\" % (epoch, test_loss, test_r2))\n",
        "        print(\"Finish with for one epoch:{} second\".format(end - start))\n",
        "    return model"
      ],
      "metadata": {
        "id": "rQPC1AyF9zAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r2_loss(output, target):\n",
        "    target_mean = torch.mean(target)\n",
        "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
        "    ss_res = torch.sum((target - output) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2"
      ],
      "metadata": {
        "id": "rZN0BF-290E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mse_r2_static(model,loss_fn, dataloader,DEVICE):\n",
        "    model.eval()\n",
        "    loss_test_cum = 0\n",
        "    y_test_pred = []\n",
        "    y_test_label = []\n",
        "    with torch.no_grad():\n",
        "        for i, (x, z, y) in enumerate(dataloader):\n",
        "            y_pred = model(x.to(DEVICE),z.to(DEVICE))\n",
        "            # predicted = model(input)\n",
        "            loss   = loss_fn(y_pred, y.to(DEVICE).squeeze())\n",
        "            loss_test_cum += loss\n",
        "            y_test_pred.append(y_pred)\n",
        "            y_test_label.append(y.to(DEVICE).squeeze())\n",
        "        r2 = r2_loss(torch.cat(y_test_pred),torch.cat(y_test_label))\n",
        "    return loss_test_cum/(i+1),r2"
      ],
      "metadata": {
        "id": "bEFVR7_1919p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (climate, yield_label, soil) = load('maize','train',mypath)\n",
        "static_var=['nitrogen','texture_class','lon','lat','yield_mean','tas', 'tasmax', 'tasmin', 'pr', 'rsds']"
      ],
      "metadata": {
        "id": "Jw_b5Fwk927s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make maize train_loader\n",
        "train_loader,val_loader,test_loader = get_training_dataloader('maize','train',mypath,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "hH_aA31B94Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x,z,y) = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "8B6gHGua95Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global model training\n",
        "model = LSTMForecaster_static(n_features = x.shape[2], n_hidden = 200, n_outputs = 1,\n",
        "                              n_static = len(static_var)+1,n_static_hidden=200,sequence_len = x.shape[1],\n",
        "                              DEVICE = DEVICE, n_deep_layers=4, use_cuda=USE_CUDA, dropout=0.4).to(DEVICE)\n",
        "\n",
        "maize_unscale_model_GLOB = train_model(model, train_loader,val_loader,test_loader,static_var,DEVICE,\n",
        "                                  lr=4e-4, weight_decay=0.1,step_size=100,sch_gamma=0.8,num_epochs=200)"
      ],
      "metadata": {
        "id": "Ek52qoiE95-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make wheat train_loader\n",
        "train_loader,val_loader,test_loader = get_training_dataloader('wheat','train',mypath,\n",
        "                                                              region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "q0026HiP98DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x,z,y) = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "lOTJCJoa99Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global model training\n",
        "model = LSTMForecaster_static(n_features = x.shape[2], n_hidden = 200, n_outputs = 1,\n",
        "                              n_static = len(static_var)+1,n_static_hidden=200,sequence_len = x.shape[1],\n",
        "                              DEVICE = DEVICE, n_deep_layers=4, use_cuda=USE_CUDA, dropout=0.4).to(DEVICE)\n",
        "\n",
        "wheat_unscale_model_GLOB2 = train_model(model, train_loader,val_loader,test_loader,static_var,DEVICE,\n",
        "                                  lr=4e-4, weight_decay=0.1,step_size=100,sch_gamma=0.8,num_epochs=200)"
      ],
      "metadata": {
        "id": "LW_P5OCJ99tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_dataloader(climate,climate_test,soil,soil_test,region,static_var):\n",
        "    # Reshape and calculate mean and sd along the grouped axis\n",
        "    climate_test_mean = climate_test.reshape(climate_test.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_test_sd = climate_test.reshape(climate_test.shape[0], 8, 30, 5).std(axis=2)\n",
        "    GPP_test = np.maximum(climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2,keepdims=True).cumsum(axis=1) # cumulative GDD by phases\n",
        "    heat_stress_day_test =  (climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    frost_days_test = (climate_test.reshape(climate_test.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    climate_test_stats = np.concatenate([climate_test_sd,climate_test_mean,GPP_test,heat_stress_day_test,frost_days_test],axis=2)\n",
        "\n",
        "    climate_mean = climate.reshape(climate.shape[0], 8, 30, 5).mean(axis=2)\n",
        "    climate_sd = climate.reshape(climate.shape[0], 8, 30, 5).std(axis=2)\n",
        "    GPP = np.maximum(climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,3] - 10, 0).sum(axis=2,keepdims=True).cumsum(axis=1) # cumulative GDD by phases\n",
        "    heat_stress_day =  (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,1] > 30).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    frost_days = (climate.reshape(climate.shape[0], 8, 30, 5)[:,:,:,2] < 0).sum(axis=2,keepdims=True).cumsum(axis=1)\n",
        "    climate_stats = np.concatenate([climate_sd,climate_mean,GPP,heat_stress_day,frost_days],axis=2)\n",
        "\n",
        "    #annual mean for test\n",
        "    annual_test_df = pd.DataFrame(climate_test.mean(axis=1),columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil_test = pd.concat([annual_test_df, soil_test], axis=1)\n",
        "\n",
        "    #annual mean for train\n",
        "    annual_df = pd.DataFrame(climate.mean(axis=1),columns=['tas', 'tasmax', 'tasmin', 'pr', 'rsds'])\n",
        "    soil = pd.concat([annual_df, soil], axis=1)\n",
        "\n",
        "    #scale input features\n",
        "    climate_arr = climate_stats.reshape(-1, climate_stats.shape[2])\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(climate_arr)\n",
        "    # scaled_climate = scaler.transform(climate_arr).reshape(climate_stats.shape)\n",
        "\n",
        "    #scale static features\n",
        "    scaler_static = StandardScaler()\n",
        "    static_arr = soil[static_var].values.astype(np.float32)\n",
        "    scaler_static.fit(static_arr)\n",
        "    # scaler_static.fit(static_arr)\n",
        "\n",
        "    climate_test_arr = climate_test_stats.reshape(-1, climate_test_stats.shape[2])\n",
        "    climate_test_scale = scaler.transform(climate_test_arr).reshape(climate_test_stats.shape)\n",
        "\n",
        "    static_test_arr = soil_test[static_var].values.astype(np.float32)\n",
        "    static_test = scaler_static.transform(static_test_arr)\n",
        "    #add co2\n",
        "    soil_test['co2'] = soil_test['co2']/1500\n",
        "    static_test = np.concatenate([static_test,soil_test['co2'].values.reshape(-1, 1)],axis = 1)\n",
        "\n",
        "    X_test_to_loader = torch.from_numpy(climate_test_scale).type(torch.FloatTensor)\n",
        "    static_test_loader = torch.from_numpy(static_test).type(torch.FloatTensor)\n",
        "\n",
        "    dataset = TensorDataset(X_test_to_loader, static_test_loader)\n",
        "\n",
        "    # subset by REGION\n",
        "    Region_index = soil_test[soil_test['region'].isin(region)].index.values\n",
        "    dataset = torch.utils.data.Subset(dataset, Region_index)\n",
        "    print(len(dataset))\n",
        "\n",
        "    test_loader_stats = torch.utils.data.DataLoader(dataset, 2000, shuffle=False)\n",
        "\n",
        "    return soil_test, Region_index, test_loader_stats"
      ],
      "metadata": {
        "id": "eGT88ueZ998k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make maize test_loader\n",
        "crop = 'maize'\n",
        "(climate, yield_label, soil) = load(crop,'train', mypath)\n",
        "(climate_test, yield_, soil_test_maize) = load(crop,'test', mypath)"
      ],
      "metadata": {
        "id": "g_4j-MYU-A9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detrend yield\n",
        "soil = soil.reset_index()\n",
        "soil['yield'] = yield_label\n",
        "grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil_test_maize = soil_test_maize.reset_index()\n",
        "soil_test_maize = pd.merge(soil_test_maize, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil['de_yield'] = soil['yield']-soil['yield_mean']\n",
        "soil['region'] = soil.apply(get_region, axis=1)\n",
        "soil_test_maize['region'] = soil_test_maize.apply(get_region, axis=1)"
      ],
      "metadata": {
        "id": "8nTzaHfT-DkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_soil, Region_index_US, test_loader_US = get_test_dataloader(climate,climate_test,soil,soil_test_maize,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "0SqoCA0X-Ese"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maize_pred_stats_test = []\n",
        "for i, (x,z) in enumerate(test_loader_US):\n",
        "    with torch.no_grad():\n",
        "        # y_pred = maize_unscale_model_US(x.to(DEVICE),z.to(DEVICE))\n",
        "        y_pred = maize_unscale_model_GLOB(x.to(DEVICE),z.to(DEVICE))\n",
        "        maize_pred_stats_test.append(y_pred.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "uX-PU2Yr-F5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## make wheat test_loader\n",
        "crop = 'wheat'\n",
        "(climate, yield_label, soil) = load(crop,'train', mypath)\n",
        "(climate_test, yield_, soil_test_wheat) = load(crop,'test', mypath)"
      ],
      "metadata": {
        "id": "GEVDaxjV-HII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detrend yield\n",
        "soil = soil.reset_index()\n",
        "soil['yield'] = yield_label\n",
        "grouped_mean = soil.groupby(['lon', 'lat'])['yield'].mean().reset_index()\n",
        "grouped_mean = grouped_mean.rename(columns={'yield': 'yield_mean'})\n",
        "soil = pd.merge(soil, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil_test_wheat = soil_test_wheat.reset_index()\n",
        "soil_test_wheat = pd.merge(soil_test_wheat, grouped_mean, on=['lon', 'lat'])\n",
        "\n",
        "soil['de_yield'] = soil['yield']-soil['yield_mean']\n",
        "soil['region'] = soil.apply(get_region, axis=1)\n",
        "soil_test_wheat['region'] = soil_test_wheat.apply(get_region, axis=1)"
      ],
      "metadata": {
        "id": "JF2zIRkO-H4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_soil, Region_index_US, test_loader_US = get_test_dataloader(climate,climate_test,soil,soil_test_wheat,region=['USA', 'Other', 'South America', 'Europe'],static_var=static_var)"
      ],
      "metadata": {
        "id": "KzA0Qr2v-Jrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_pred_stats_test = []\n",
        "for i, (x,z) in enumerate(test_loader_US):\n",
        "    with torch.no_grad():\n",
        "        # y_pred = maize_unscale_model_US(x.to(DEVICE),z.to(DEVICE))\n",
        "        y_pred = wheat_unscale_model_GLOB2(x.to(DEVICE),z.to(DEVICE))\n",
        "        wheat_pred_stats_test.append(y_pred.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "CrJPtah6-NYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(soil_test_maize['preds_yield'], bins=50, alpha=0.6, label='Maize')\n",
        "plt.hist(soil_test_wheat['preds_yield'], bins=50, alpha=0.6, label='Wheat')\n",
        "plt.xlabel('Predicted Yield')\n",
        "plt.ylabel('Number of Grid Cells')\n",
        "plt.title('Distribution of Predicted Yields (Test Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JkNrHeZc-O9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(soil_test_maize['lon'], soil_test_maize['lat'],\n",
        "            c=soil_test_maize['preds_yield'],  s=20, alpha=0.7)\n",
        "plt.colorbar(label='Predicted Maize Yield')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Spatial Distribution of Predicted Maize Yields')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_TkLfkI-Pt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "static_var=['nitrogen','texture_class','lon','lat','yield_mean','tas', 'tasmax', 'tasmin', 'pr', 'rsds']\n",
        "\n",
        "# For Maize\n",
        "_, _, maize_test_loader = get_training_dataloader(\n",
        "    'maize', 'train', mypath,\n",
        "    region=['USA', 'Other', 'South America', 'Europe'],\n",
        "    static_var=static_var\n",
        ")\n",
        "\n",
        "# For Wheat\n",
        "_, _, wheat_test_loader = get_training_dataloader(\n",
        "    'wheat', 'train', mypath,\n",
        "    region=['USA', 'Other', 'South America', 'Europe'],\n",
        "    static_var=static_var\n",
        ")"
      ],
      "metadata": {
        "id": "yUERwebM-RKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def evaluate_lstm(model, test_loader, title_prefix=\"\"):\n",
        "    model.eval()\n",
        "    y_true_all, y_pred_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, x_static, y_true in test_loader:\n",
        "            y_pred = model(x.to(DEVICE), x_static.to(DEVICE)).cpu().numpy()\n",
        "            y_true = y_true.cpu().numpy()\n",
        "            y_true_all.append(y_true)\n",
        "            y_pred_all.append(y_pred)\n",
        "\n",
        "    y_true_all = np.concatenate(y_true_all)\n",
        "    y_pred_all = np.concatenate(y_pred_all)\n",
        "    residuals = y_true_all - y_pred_all\n",
        "    r2 = r2_score(y_true_all, y_pred_all)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
        "\n",
        "    return y_true_all, y_pred_all, residuals, r2, rmse\n",
        "\n",
        "# --- Evaluate both models ---\n",
        "y_true_wheat, y_pred_wheat, resid_wheat, r2_wheat, rmse_wheat = evaluate_lstm(wheat_unscale_model_GLOB2, wheat_test_loader, \"Wheat\")\n",
        "y_true_maize, y_pred_maize, resid_maize, r2_maize, rmse_maize = evaluate_lstm(maize_unscale_model_GLOB, maize_test_loader, \"Maize\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "# 1. Wheat: Pred vs Actual\n",
        "axs[0, 0].scatter(y_true_wheat, y_pred_wheat, alpha=0.5, s=15, edgecolor='k')\n",
        "axs[0, 0].plot([y_true_wheat.min(), y_true_wheat.max()],\n",
        "               [y_true_wheat.min(), y_true_wheat.max()],\n",
        "               'r--', label='Perfect Prediction')\n",
        "axs[0, 0].set_title(f'LSTM Predicted vs Actual Wheat Yield\\nR² = {r2_wheat:.4f}, RMSE = {rmse_wheat:.3f}')\n",
        "axs[0, 0].set_xlabel(\"Actual Yield\")\n",
        "axs[0, 0].set_ylabel(\"Predicted Yield\")\n",
        "axs[0, 0].legend()\n",
        "axs[0, 0].grid(True)\n",
        "\n",
        "# 2. Maize: Pred vs Actual\n",
        "axs[0, 1].scatter(y_true_maize, y_pred_maize, alpha=0.5, s=15, edgecolor='k')\n",
        "axs[0, 1].plot([y_true_maize.min(), y_true_maize.max()],\n",
        "               [y_true_maize.min(), y_true_maize.max()],\n",
        "               'r--', label='Perfect Prediction')\n",
        "axs[0, 1].set_title(f'LSTM Predicted vs Actual Maize Yield\\nR² = {r2_maize:.4f}, RMSE = {rmse_maize:.3f}')\n",
        "axs[0, 1].set_xlabel(\"Actual Yield\")\n",
        "axs[0, 1].set_ylabel(\"Predicted Yield\")\n",
        "axs[0, 1].legend()\n",
        "axs[0, 1].grid(True)\n",
        "\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# 3.\n",
        "resid_w = resid_wheat.ravel()\n",
        "mean_w, std_w = np.mean(resid_w), np.std(resid_w)\n",
        "x_w = np.linspace(resid_w.min(), resid_w.max(), 500)\n",
        "pdf_w = (1 / (std_w * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_w - mean_w) / std_w) ** 2)\n",
        "\n",
        "axs[1, 0].hist(resid_w, bins=50, color='forestgreen', edgecolor='black', alpha=0.7, density=True)\n",
        "axs[1, 0].set_title('LSTM Residuals (Wheat): Actual - Predicted')\n",
        "axs[1, 0].set_xlabel(\"Residual\")\n",
        "axs[1, 0].set_ylabel(\"Density\")\n",
        "axs[1, 0].legend()\n",
        "axs[1, 0].grid(True)\n",
        "\n",
        "# 4.\n",
        "resid_m = resid_maize.ravel()\n",
        "mean_m, std_m = np.mean(resid_m), np.std(resid_m)\n",
        "x_m = np.linspace(resid_m.min(), resid_m.max(), 500)\n",
        "pdf_m = (1 / (std_m * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_m - mean_m) / std_m) ** 2)\n",
        "\n",
        "axs[1, 1].hist(resid_m, bins=50, color='darkgreen', edgecolor='black', alpha=0.7, density=True)\n",
        "axs[1, 1].set_title('LSTM Residuals (Maize): Actual - Predicted')\n",
        "axs[1, 1].set_xlabel(\"Residual\")\n",
        "axs[1, 1].set_ylabel(\"Density\")\n",
        "axs[1, 1].legend()\n",
        "axs[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gWwl5PWp-S0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM with Raw Data"
      ],
      "metadata": {
        "id": "BTKc7AP6-Wkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "F4cBJSHE-ZpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xdtgJNSV-cID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = '/content/drive/MyDrive/the-future-crop-challenge'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on\", device)"
      ],
      "metadata": {
        "id": "ECtgBJ0o-dDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount & imports\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 3) List all the maize‐train parquet files\n",
        "all_files = sorted(fn\n",
        "                   for fn in os.listdir(BASE)\n",
        "                   if fn.endswith('_maize_train.parquet'))\n",
        "print(\"Found maize‐train files:\\n\", all_files)\n",
        "\n",
        "# 4) Load yields (train_solutions) and reset ID from index → column\n",
        "yld = pd.read_parquet(os.path.join(BASE, 'train_solutions_maize.parquet'))\n",
        "if yld.index.name == 'ID':\n",
        "    yld = yld.reset_index()\n",
        "print(\"Yields columns:\", yld.columns.tolist())\n",
        "\n",
        "# 5) Load soil+CO₂ and reset ID\n",
        "soil_fn = [f for f in all_files if f.startswith('soil_co2')][0]\n",
        "soil   = pd.read_parquet(os.path.join(BASE, soil_fn))\n",
        "if soil.index.name == 'ID':\n",
        "    soil = soil.reset_index()\n",
        "print(\"Soil columns:\", soil.columns.tolist())\n",
        "\n",
        "# 6) Merge yields + soil on ID\n",
        "data = yld.merge(\n",
        "    soil[['ID','real_year','texture_class','nitrogen','co2']],\n",
        "    on='ID', how='left'\n",
        ")\n",
        "print(\"After soil merge:\", data.shape)\n",
        "\n",
        "# 7) Pick out just the climate files (pr, rsds, tas, tasmax, tasmin)\n",
        "climate_files = [f for f in all_files\n",
        "                 if f.split('_')[0] in {'pr','rsds','tas','tasmax','tasmin'}]\n",
        "print(\"Will merge these climate files:\\n\", climate_files)\n",
        "\n",
        "# 8) Loop & merge, renaming day‑cols to <var>_<day> so there are no dupes\n",
        "for fn in climate_files:\n",
        "    var = fn.split('_')[0]                 # 'pr' or 'rsds' or 'tas'/'tasmax'/'tasmin'\n",
        "    df  = pd.read_parquet(os.path.join(BASE, fn))\n",
        "    if df.index.name == 'ID':\n",
        "        df = df.reset_index()\n",
        "\n",
        "    # day‑columns are named '0','1',…'239'\n",
        "    day_cols = [c for c in df.columns if c.isdigit()]\n",
        "    # build rename map so pr‑days become pr_0, pr_1, … etc.\n",
        "    rename_map = {c: f\"{var}_{c}\" for c in day_cols}\n",
        "\n",
        "    # slice out ID + day‑cols, rename, and merge\n",
        "    df_days = df[['ID'] + day_cols].rename(columns=rename_map)\n",
        "    print(f\"Merging {fn!r}: {len(day_cols)} days → new cols prefix '{var}_'\")\n",
        "    data = data.merge(df_days, on='ID', how='left')\n",
        "\n",
        "# 9) Check final size\n",
        "print(\"Final data shape:\", data.shape)\n",
        "print(\"Sample of all columns:\", data.columns[:12].tolist(), \"…\")"
      ],
      "metadata": {
        "id": "HYGn0e3Y-eAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1) Train/validation split by real_year\n",
        "train_df = data[data.real_year < 2020].reset_index(drop=True)\n",
        "val_df   = data[data.real_year == 2020].reset_index(drop=True)\n",
        "\n",
        "# 2) Extract target arrays\n",
        "y_train = train_df['yield'].values.astype(np.float32)\n",
        "y_val   = val_df  ['yield'].values.astype(np.float32)\n",
        "\n",
        "# 3) Build dynamic (time‑series) feature arrays\n",
        "clim_vars = ['pr', 'rsds', 'tas', 'tasmax', 'tasmin']\n",
        "dyn_cols = []\n",
        "for var in clim_vars:\n",
        "    dyn_cols += [c for c in train_df.columns if c.startswith(f\"{var}_\")]\n",
        "\n",
        "X_train_dyn = train_df[dyn_cols].values.astype(np.float32)\n",
        "X_val_dyn   = val_df  [dyn_cols].values.astype(np.float32)\n",
        "\n",
        "# 4) Scale and reshape dynamic features → (samples, 240, 5)\n",
        "scaler_dyn   = StandardScaler()\n",
        "X_train_dyn  = scaler_dyn.fit_transform(X_train_dyn)\n",
        "X_val_dyn    = scaler_dyn.transform(X_val_dyn)\n",
        "X_train_dyn  = X_train_dyn.reshape(-1, 240, len(clim_vars))\n",
        "X_val_dyn    = X_val_dyn.reshape(-1, 240, len(clim_vars))\n",
        "\n",
        "# 5) Build static feature arrays and scale → (samples, 3)\n",
        "stat_cols   = ['texture_class', 'nitrogen', 'co2']\n",
        "X_train_stat = train_df[stat_cols].values.astype(np.float32)\n",
        "X_val_stat   = val_df  [stat_cols].values.astype(np.float32)\n",
        "\n",
        "scaler_stat  = StandardScaler()\n",
        "X_train_stat = scaler_stat.fit_transform(X_train_stat)\n",
        "X_val_stat   = scaler_stat.transform(X_val_stat)\n",
        "\n",
        "# 6) Define a PyTorch Dataset\n",
        "class CropDataset(Dataset):\n",
        "    def __init__(self, Xdyn, Xstat, y):\n",
        "        self.Xdyn  = torch.from_numpy(Xdyn)\n",
        "        self.Xstat = torch.from_numpy(Xstat)\n",
        "        self.y     = torch.from_numpy(y).unsqueeze(1)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.Xdyn[idx], self.Xstat[idx], self.y[idx]\n",
        "\n",
        "# 7) Instantiate datasets and dataloaders\n",
        "train_ds = CropDataset(X_train_dyn, X_train_stat, y_train)\n",
        "val_ds   = CropDataset(X_val_dyn,   X_val_stat,   y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"Train batches:\", len(train_loader),\n",
        "      \"| Val batches:\", len(val_loader))"
      ],
      "metadata": {
        "id": "Fx3-lU93-fO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "# 1) Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 2) Model definition\n",
        "class CropLSTM(nn.Module):\n",
        "    def __init__(self, n_dyn_feats=5, seq_len=240, n_stat=3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=n_dyn_feats, hidden_size=64,\n",
        "                            batch_first=True)\n",
        "        self.dyn_fc = nn.Sequential(nn.Linear(64, 32), nn.ReLU())\n",
        "        self.stat_fc = nn.Sequential(nn.Linear(n_stat, 16), nn.ReLU())\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(32 + 16, 16), nn.ReLU(),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_dyn, x_stat):\n",
        "        _, (h_n, _) = self.lstm(x_dyn)      # (1, B, 64)\n",
        "        h = h_n.squeeze(0)                  # → (B, 64)\n",
        "        d = self.dyn_fc(h)                  # → (B, 32)\n",
        "        s = self.stat_fc(x_stat)            # → (B, 16)\n",
        "        return self.head(torch.cat([d, s], dim=1))\n",
        "\n",
        "model = CropLSTM(n_dyn_feats=len(clim_vars), n_stat=len(stat_cols)).to(device)\n",
        "\n",
        "# 3) Loss & optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 4) Training loop with tqdm & R²\n",
        "best_val_loss = float('inf')\n",
        "patience, wait = 5, 0\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    # —— Train ——\n",
        "    model.train()\n",
        "    train_losses, train_preds, train_trues = [], [], []\n",
        "    for Xd, Xs, y in tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False):\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(Xd, Xs)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item() * y.size(0))\n",
        "        train_preds.append(out.detach().cpu())\n",
        "        train_trues.append(y.cpu())\n",
        "\n",
        "    train_mse = sum(train_losses) / len(train_loader.dataset)\n",
        "    train_r2  = r2_score(\n",
        "        torch.cat(train_trues).numpy(),\n",
        "        torch.cat(train_preds).numpy()\n",
        "    )\n",
        "\n",
        "    # —— Validate ——\n",
        "    model.eval()\n",
        "    val_losses, val_preds, val_trues = [], [], []\n",
        "    for Xd, Xs, y in tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False):\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        out = model(Xd, Xs)\n",
        "        val_losses.append(criterion(out, y).item() * y.size(0))\n",
        "        val_preds.append(out.detach().cpu())\n",
        "        val_trues.append(y.cpu())\n",
        "\n",
        "    val_mse = sum(val_losses) / len(val_loader.dataset)\n",
        "    val_r2  = r2_score(\n",
        "        torch.cat(val_trues).numpy(),\n",
        "        torch.cat(val_preds).numpy()\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"Train MSE={train_mse:.4f} | Train R²={train_r2:.4f} | \"\n",
        "          f\"Val MSE={val_mse:.4f} | Val R²={val_r2:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_mse < best_val_loss:\n",
        "        best_val_loss, wait = val_mse, 0\n",
        "        torch.save(model.state_dict(), 'best_crop_lstm.pth')\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# 5) Final 2020 evaluation\n",
        "model.load_state_dict(torch.load('best_crop_lstm.pth'))\n",
        "model.eval()\n",
        "all_preds, all_trues = [], []\n",
        "with torch.no_grad():\n",
        "    for Xd, Xs, y in val_loader:\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        out = model(Xd, Xs)\n",
        "        all_preds.append(out.cpu())\n",
        "        all_trues.append(y.cpu())\n",
        "\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_trues = torch.cat(all_trues).numpy()\n",
        "final_mse = ((all_preds - all_trues)**2).mean()\n",
        "final_r2  = r2_score(all_trues, all_preds)\n",
        "\n",
        "print(f\"\\nFinal 2020 RMSE: {final_mse**0.5:.4f} | Final 2020 R²: {final_r2:.4f}\")"
      ],
      "metadata": {
        "id": "aSyWoXLM-hDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline MSE of Prediciting Mean of Maize"
      ],
      "metadata": {
        "id": "h5T6hh3F-o3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y = train_df['yield'].values\n",
        "baseline_mse  = np.var(y)                   # MSE of always‑predicting y.mean()\n",
        "baseline_rmse = np.sqrt(baseline_mse)\n",
        "print(f\"Baseline RMSE = {baseline_rmse:.3f}, MSE = {baseline_mse:.3f}\")"
      ],
      "metadata": {
        "id": "90awgpvx-oWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wheat"
      ],
      "metadata": {
        "id": "au9B1qnX-tmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, pandas as pd, numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# 2) Set crop type\n",
        "crop = 'wheat'\n",
        "\n",
        "# 3) Discover available wheat‐train files\n",
        "all_files = sorted(fn for fn in os.listdir(BASE) if fn.endswith(f'_{crop}_train.parquet'))\n",
        "print(\"Available wheat‐train files:\", all_files)\n",
        "\n",
        "# 4) Load yields & reset index\n",
        "yld = pd.read_parquet(os.path.join(BASE, f'train_solutions_{crop}.parquet'))\n",
        "if yld.index.name == 'ID':\n",
        "    yld = yld.reset_index()\n",
        "print(\"Yields columns:\", yld.columns.tolist())\n",
        "\n",
        "# 5) Load soil/CO₂ & reset index\n",
        "soil = pd.read_parquet(os.path.join(BASE, f'soil_co2_{crop}_train.parquet'))\n",
        "if soil.index.name == 'ID':\n",
        "    soil = soil.reset_index()\n",
        "print(\"Soil columns:\", soil.columns.tolist())\n",
        "\n",
        "# 6) Merge yields + soil\n",
        "data = yld.merge(\n",
        "    soil[['ID','real_year','texture_class','nitrogen','co2']],\n",
        "    on='ID', how='left'\n",
        ")\n",
        "print(\"After soil merge:\", data.shape)\n",
        "\n",
        "# 7) Merge each climate series (pr, rsds, tas, tasmax, tasmin)\n",
        "clim_vars = ['pr','rsds','tas','tasmax','tasmin']\n",
        "for var in clim_vars:\n",
        "    fn = f\"{var}_{crop}_train.parquet\"\n",
        "    df = pd.read_parquet(os.path.join(BASE, fn))\n",
        "    if df.index.name == 'ID':\n",
        "        df = df.reset_index()\n",
        "    day_cols = [c for c in df.columns if c.isdigit()]\n",
        "    # rename '0'→'pr_0', etc.\n",
        "    rename_map = {c: f\"{var}_{c}\" for c in day_cols}\n",
        "    df_days = df[['ID'] + day_cols].rename(columns=rename_map)\n",
        "    data = data.merge(df_days, on='ID', how='left')\n",
        "    print(f\"Merged {fn} → now data shape {data.shape}\")\n",
        "\n",
        "# 8) Train/val split by real_year\n",
        "train_df = data[data.real_year < 2020].reset_index(drop=True)\n",
        "val_df   = data[data.real_year == 2020].reset_index(drop=True)\n",
        "\n",
        "# 9) Targets\n",
        "y_train = train_df['yield'].values.astype(np.float32)\n",
        "y_val   = val_df  ['yield'].values.astype(np.float32)\n",
        "\n",
        "# 10) Dynamic (\"time‑series\") features\n",
        "dyn_cols = [c for var in clim_vars for c in train_df.columns if c.startswith(f\"{var}_\")]\n",
        "Xtr_dyn = train_df[dyn_cols].values.astype(np.float32)\n",
        "Xvl_dyn = val_df  [dyn_cols].values.astype(np.float32)\n",
        "\n",
        "scaler_dyn = StandardScaler()\n",
        "Xtr_dyn = scaler_dyn.fit_transform(Xtr_dyn).reshape(-1, 240, len(clim_vars))\n",
        "Xvl_dyn = scaler_dyn.transform(Xvl_dyn).reshape(-1, 240, len(clim_vars))\n",
        "\n",
        "# 11) Static features\n",
        "stat_cols = ['texture_class','nitrogen','co2']\n",
        "Xtr_stat  = train_df[stat_cols].values.astype(np.float32)\n",
        "Xvl_stat  = val_df  [stat_cols].values.astype(np.float32)\n",
        "\n",
        "scaler_stat = StandardScaler()\n",
        "Xtr_stat = scaler_stat.fit_transform(Xtr_stat)\n",
        "Xvl_stat = scaler_stat.transform(Xvl_stat)\n",
        "\n",
        "# 12) PyTorch Dataset & DataLoader\n",
        "class WheatDataset(Dataset):\n",
        "    def __init__(self, Xdyn, Xstat, y):\n",
        "        self.Xdyn  = torch.from_numpy(Xdyn)\n",
        "        self.Xstat = torch.from_numpy(Xstat)\n",
        "        self.y     = torch.from_numpy(y).unsqueeze(1)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.Xdyn[idx], self.Xstat[idx], self.y[idx]\n",
        "\n",
        "train_ds = WheatDataset(Xtr_dyn, Xtr_stat, y_train)\n",
        "val_ds   = WheatDataset(Xvl_dyn, Xvl_stat, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Wheat train batches:\", len(train_loader), \"| Wheat val batches:\", len(val_loader))"
      ],
      "metadata": {
        "id": "tZTmqsPz-i7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ——— 1) Device ———\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ——— 2) Model ———\n",
        "# Re‑use the CropLSTM class you defined earlier\n",
        "model = CropLSTM(n_dyn_feats=len(clim_vars), n_stat=len(stat_cols)).to(device)\n",
        "\n",
        "# ——— 3) Loss & Optimizer ———\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ——— 4) Training loop with early stopping & R² ———\n",
        "best_val_loss = float('inf')\n",
        "patience, wait = 5, 0\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    train_losses, train_preds, train_trues = [], [], []\n",
        "    for Xd, Xs, y in tqdm(train_loader, desc=f\"Epoch {epoch} Train\", leave=False):\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(Xd, Xs)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item() * y.size(0))\n",
        "        train_preds.append(out.detach().cpu())\n",
        "        train_trues.append(y.cpu())\n",
        "\n",
        "    train_mse = sum(train_losses) / len(train_loader.dataset)\n",
        "    train_r2  = r2_score(\n",
        "        torch.cat(train_trues).numpy(),\n",
        "        torch.cat(train_preds).numpy()\n",
        "    )\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    val_losses, val_preds, val_trues = [], [], []\n",
        "    for Xd, Xs, y in tqdm(val_loader, desc=f\"Epoch {epoch} Val\", leave=False):\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        out = model(Xd, Xs)\n",
        "        val_losses.append(criterion(out, y).item() * y.size(0))\n",
        "        val_preds.append(out.detach().cpu())\n",
        "        val_trues.append(y.cpu())\n",
        "\n",
        "    val_mse = sum(val_losses) / len(val_loader.dataset)\n",
        "    val_r2  = r2_score(\n",
        "        torch.cat(val_trues).numpy(),\n",
        "        torch.cat(val_preds).numpy()\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"Train MSE={train_mse:.4f} | Train R²={train_r2:.4f} | \"\n",
        "        f\"Val MSE={val_mse:.4f} | Val R²={val_r2:.4f}\"\n",
        "    )\n",
        "\n",
        "    # — Early stopping —\n",
        "    if val_mse < best_val_loss:\n",
        "        best_val_loss, wait = val_mse, 0\n",
        "        torch.save(model.state_dict(), 'best_wheat_lstm.pth')\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# ——— 5) Final hold‑out evaluation on 2020 ———\n",
        "model.load_state_dict(torch.load('best_wheat_lstm.pth'))\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_trues = [], []\n",
        "with torch.no_grad():\n",
        "    for Xd, Xs, y in val_loader:\n",
        "        Xd, Xs, y = Xd.to(device), Xs.to(device), y.to(device)\n",
        "        out = model(Xd, Xs)\n",
        "        all_preds.append(out.cpu())\n",
        "        all_trues.append(y.cpu())\n",
        "\n",
        "all_preds = torch.cat(all_preds).numpy().ravel()\n",
        "all_trues = torch.cat(all_trues).numpy().ravel()\n",
        "\n",
        "final_mse = ((all_preds - all_trues)**2).mean()\n",
        "final_r2  = r2_score(all_trues, all_preds)\n",
        "\n",
        "print(f\"\\nFinal 2020 RMSE: {final_mse**0.5:.4f} | Final 2020 R²: {final_r2:.4f}\")"
      ],
      "metadata": {
        "id": "mfMuYKWO-1Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We check for the baseline RMSE for wheat as well\n",
        "import numpy as np\n",
        "\n",
        "# 1) Obtain training yields\n",
        "y_train = train_df['yield'].values.astype(np.float32)\n",
        "\n",
        "# 2) Compute baseline MSE & RMSE (predicting the training mean)\n",
        "baseline_mse  = np.var(y_train)\n",
        "baseline_rmse = np.sqrt(baseline_mse)\n",
        "\n",
        "# 3) Print\n",
        "print(f\"Baseline MSE (wheat)  = {baseline_mse:.4f}\")\n",
        "print(f\"Baseline RMSE (wheat) = {baseline_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "e2GEvM5a-1s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "hkBRBN-s-9qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Predicted vs Actual Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=all_trues, y=all_preds, alpha=0.6, edgecolor='k', s=30)\n",
        "plt.plot([all_trues.min(), all_trues.max()],\n",
        "         [all_trues.min(), all_trues.max()],\n",
        "         'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield (2020)\")\n",
        "plt.ylabel(\"Predicted Yield (2020)\")\n",
        "plt.title(f\"Predicted vs Actual Wheat Yield (2020)\\nR² = {final_r2:.4f}, RMSE = {final_mse**0.5:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Residuals Histogram\n",
        "residuals = all_trues - all_preds\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(residuals, bins=50, kde=True, color='forestgreen')\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals Distribution (Actual - Predicted): wheat\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ogsg0Sk6-59p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "\n",
        "maize_preds = all_preds.copy()\n",
        "maize_trues = all_trues.copy()\n",
        "\n",
        "# Compute R² and RMSE\n",
        "final_r2_maize = r2_score(maize_trues, maize_preds)\n",
        "final_rmse_maize = mean_squared_error(maize_trues, maize_preds)\n",
        "\n",
        "# 1. Predicted vs Actual Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=maize_trues, y=maize_preds, alpha=0.6, edgecolor='k', s=30)\n",
        "plt.plot([maize_trues.min(), maize_trues.max()],\n",
        "         [maize_trues.min(), maize_trues.max()],\n",
        "         'r--', label='Perfect Prediction')\n",
        "plt.xlabel(\"Actual Yield (2020)\")\n",
        "plt.ylabel(\"Predicted Yield (2020)\")\n",
        "plt.title(f\"Predicted vs Actual Maize Yield (2020)\\nR² = {final_r2_maize:.4f}, RMSE = {final_rmse_maize:.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Residuals Histogram\n",
        "residuals_maize = maize_trues - maize_preds\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(residuals_maize, bins=50, kde=True, color='cornflowerblue')\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals Distribution (Actual - Predicted): Maize\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MlTwbjIwA5CD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
